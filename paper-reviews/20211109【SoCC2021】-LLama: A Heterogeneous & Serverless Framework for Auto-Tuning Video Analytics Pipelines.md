By AoDong Chen
# LLAMA:一个能够对视频分析流水线进行自动调优的异构无服务器框架
[LLama: A Heterogeneous & Serverless Framework for
Auto-Tuning Video Analytics Pipelines](https://arxiv.org/pdf/2102.01887.pdf)


本文提出了LLAMA:一个能够对视频分析流水线进行自动调优的异构无服务器框架。给定一个端到端延迟目标，LLAMA通过首先为视频分析流水线中的每个操作的调用计算一个延迟目标，然后在线运行一个以成本为优化目标的优化器来确定配置，这些配置能最好地满足前面计算出的每个调用的延迟目标。这使得自动调优大型视频流水线的问题变得容易处理，而且可以处理依赖于输入的现象、DAG中的条件分支和执行过程的多变性。文章介绍了LLAMA中的算法，并在云平台上使用无服务器CPU和GPU资源对其进行评估。  

结果表明，与最先进的集群和无服务器视频分析、处理系统相比，LLAMA实现了7.8×的延迟降低和16×成本减少。  

## 论文作者
Francisco Romero：斯坦福大学的博士研究生。研究方向主要是视频分析、机器学习推理、和无服务计算。  

Mark Zhao：斯坦福大学的博士研究生。

Neeraja J. Yadwadkar：斯坦福大学计算机科学系的博士后研究员。研究方向主要是在系统中使用并开发机器学习技术，同时研究机器学习系统。  

Christos Kozyrakis：斯坦福大学电气工程和计算机科学教授。研究方向主要是资源高效的云计算、用于新兴工作负载的节能计算和内存系统，以及可扩展的操作系统。

## 论文动机
视频流量的规模正在爆炸性增长，预计到2022年将占互联网总流量的82%以上。大量的领域都会应用视频处理流水线技术，该技术通过数十种视频分析和处理操作，从原始视频中提取有意义的信息。而视频流水线必须满足多种多样的延迟、吞吐量和成本要求，才能在应用程序中实用。因此视频分析和处理框架必须合理调整流水线操作参数配置（例如，采样率、批量大小、硬件目标和资源分配）以满足不同应用程序的延迟或成本要求。但是，由于视频处理操作在不同硬件上执行的性能有差异，流水线的具体流程可能依赖于中间步骤，因此很难自动调整每一个处理操作的参数配置。

现存的基于无服务器的视频流水线系统只能支持静态配置参数和顺序确定的流水线流程，无法自动的针对不同的目标要求配置不同的资源。而人工配置的成本又太大。 

因此论文提出了一个支持异构硬件、能够自动调整流水线中每个操作的参数配置和资源分配的视频分析处理框架。该系统主要面对的挑战有以下三点：
- 如何从为多种配置选择合适的参数（批处理大小、采样率、分辨率、硬件平台、cpu核数、gpu显存大小等）？
- 如何处理分支情况的参数配置和受多输出影响的操作的参数配置？
- 如何在流水线执行过程中动态的调整资源分配与参数设置（因为前面的操作执行完成后资源可获得量发生了变化）？

## 论文贡献
本篇论文提出了一个能够对视频分析流水线进行自动调优的异构无服务器框架。主要贡献有以下内容：
- 细粒度的配置操作。由一次性为整个流水线中的每一个操作进行配置变化为每次只对流水线中的一个操作进行配置。
- 使用了三个方法来处理依赖于中间操作的执行流程：
  - 提前预测和后期提交。当操作可调用时选择初始配置决策，然后在真正执行之前再次更新配置。
  - 基于优先级的提交策略。根据操作与硬件的匹配性和其在流水线中的深度对操作进行优先级排序。
  - 保证延时的批处理。在不违反调用分配的松弛值的情况下等待输入到来进行批处理。
- 前面两个贡献合起来能够实现动态资源分配。

### 为每个操作计算slack值（即该操作应在多少时间内完成）的算法  
![algorithm1](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/algorithm1.5pfg7w8883k0.png)

给定一个操作𝑜𝑝的调用和一个配置的后端𝜆，ComputeSlack首先通过包含𝑜𝑝的DAG找到所有顺序路径(第6行)。然后它使用每个操作的参考配置，从𝑜𝑝开始估计完成当前路径的所需时间。slack的值为当前操作的参考延时与剩余所有操作参考延时的比值乘以剩余时间。最后的slack值选取所有路径计算出的slack值中最小的那个,即考虑所有分支，选取的slack必须能够满足所有路径。

### 为单个操作选择合适的配置
![objective_function](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/objective_function.42wf3fcrw5c0.png)  
x为对应的配置

### 提前预测和后期提交
 由于后端的并行度有限，所有操作并不能马上得到运行，所以先预估操作所需的延时与相应配置，将其加入speculation队列，当真正要执行的时候，重新计算延时和对应配置加入commit队列。
排队时间的计算函数如下：  
![queue_time](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/queue_time.5y3aapdnkr00.png)

### 保证延时的批处理
当batchsize大于当前可用的调用时，在不违反slack延时的情况下，等待有足够的可用调用，否则，选择尽可能多的可用调用。  

### 基于优先级的提交策略
- 优先考虑更深的操作。
- 优先考虑对硬件适配性更高的操作。适配性计算公式如下：
  ![affinity](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/affinity.kry5hude86o.png)

## 实验
**Metrics**:流水线执行延时和成本

**基础实验设置**：  
作者在谷歌云平台(GCP)上部署了LLAMA。LLAMA运行在n1-standard-8实例(8个vcpu, 30 GB的DRAM)上。对于无服务器的CPU后端，作者使用10个n1-standard-64 (64 vcpu, 240GB DRAM)。对于无服务器的GPU后端，作者使用了2个custom-12-46080(1个V100 GPU, 12个vcpu, 45 GB DRAM)。所有实例都有Intel Xeon Platinum E5-2620 cpu, 频率为2.20GHz, Ubuntu 16.04 5.3.0，以及高达32gbps的网络速度。

**Baselines**：
- sc-cpu:仅使用cpu的Scanner
- sc-gpu:仅使用gpu的Scanner
- Nexus
- gg
- gg-branch:使用了LLAMA的分支支持的gg
- GrandSLAm++：使用LLAMA的分支支持、安全延迟批处理、基于优先级的提交以及跨异构后端动态资源定位但禁用早期推测和后期提交、反馈和深度优先级的GrandSLAm  
  

作者使用12个n1-standard-64配置了sc-cpu、gg、gg-branch和GrandSLAm++，使用了12个custom-12-46080配置了Nexus和sc-gpu

**视频流水线**
- AMBER Alert
- Face Blurring
- Denoising
- Toonify
- Synthetic
### 实验一 与现存系统的比较
#### 实验设置：
对于Nexus，作者将流水线延迟目标设置为每帧2秒，是一种最严格的延迟，能满足任何请求。然后，Nexus自动配置每个模型的批大小和实例数量。sc-cpu和sc-gpu的批处理大小范围为1到64，根据最低的流水线执行延迟设置每个值。对于gg和gg-branch，我们根据Operation-Profiler提供的最低、最经济的CPU延迟来设置每个调用的配置。作者为LLAMA和GrandSLAm++配置了两种流水线延迟目标:一个是无法实现的低延时目标，它迫使双方以成本为代价最小化流水线执行延迟:LLAMA-fast和GrandSLAm++-fast;另一种是特别宽松的延时目标，它允许LLAMA最小化总体成本:LLAMA-cheap和GrandSLAm++-cheap
#### 结论：
![figure6](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/figure6.xl5jotbtdhs.png)
![figure7](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/figure7.lzgwtubujr4.png)
图6和图7分别显示了执行四个非合成流水线的处理延迟和总成本。LLAMA比现有系统具有更低的延迟、更高的吞吐量和更低的成本。通过动态调用配置，LLAMA能够确定操作在异构后端中的表现，并根据流水线延迟的目标来确定资源的大小。

### 实验二 LLAMA能否牺牲延时来降低成本
#### 实验设置：
对于每个流水线，我们向LLAMA提供了三个延迟目标，它们位于使用LLAMA-fast和LLAMA-cheap执行流水线所需的时间之间。50%的延迟目标是LLAMA-fast和LLAMA-cheap延迟目标的平均延迟。25%的延迟目标（三个目标中最严格的一个）是LLAMA-fast和50%延迟目标之间的平均延迟。最后，75%的延迟目标（三者中最不严格的）是LLAMA-cheap和50%的延迟目标之间的平均延迟。例如，LLAMA-faste执行FaceBlurring的时间是155秒，LLAMA-cheap执行的时间是423秒；25%、50%和75%的延迟目标分别是225、290和380秒。
#### 结论：
![figure8](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/figure8.3jdvmpw050g0.png)

  

![table3](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/table3.59urpc7uc9w.png)
- 图8显示了观察到的执行延迟以及各流水线执行的原始成本值。作者对上述各流水线延迟目标进行了归一化处理（≤1表示满足延迟目标），LLAMAn不仅满足了所有的延迟目标，而且随着延迟目标变得越来越不严格，还动态地调整其配置决策以选择成本效益高的配置  
- 表3显示了用于满足50%流水线延迟目标的配置数量，以及满足松弛的调用百分比。我们注意到LLAMA在所有流水线中平均满足了94%的调用量，最低的是合成流水线，因为它是最长的；每条流水线使用的配置数量各不相同。因此，LLAMA的松弛分配和配置选择算法能够有效地满足流水线延迟目标，同时使成本最小化。  

### 实验三 LLAMA每项技术的贡献
#### 实验设置：
作者用两个不同的流水线进行了研究，分别为Amber Alert和Toonify。以下是LLAMA采用的技术清单：反馈循环，深度优先，安全延迟批处理，早期推测和后期提交，以及基于优先级的提交。请注意，基于优先级的提交包括深度优先和硬件亲和力。对于每一次运行，我们关闭一个技术，并记录流水线的执行延迟和成本。对于每个流水线，我们使用50%的流水线延迟目标。

#### 结论：

![figure9](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/figure9.7kos1ep88qc0.png)


图9显示了我们取消某项技术研究的结果（红色边框和圈出的斜线表示违反了延迟目标)。对于AMBER Alert流水线，禁用反馈、深度优先或早期投机和晚期提交会导致违反延迟目标。对于Toonify流水线，禁用反馈也会导致与AMBER Alert流水线类似的违反延迟目标的情况。禁用安全延迟批处理或早期投机和晚期提交都会导致LLAMA选择成本效率较低的配置。另一方面，禁用深度优先和基于优先级的提交会导致更具成本效率的配置，而不违反延迟目标。

### 实验四 LLAMA存在分析错误和执行失败，但仍能满足目标延时
#### 实验设置：
为了评估 "错误分析"，所有操作分析得到的延迟被设置为正确值的50%。另外，为了评估LLAMA对失败的适应性，我们强迫3%的调用失败（对Denoising和Synthetic流水线来说，分别有2,114和3,617次失败）。在这两个实验中，我们使用了去噪和合成流水线，因为它们拥有流水线中最糟糕的情况：在流水线的末端有一个费时的操作，并且有一个被低估的latency。此外，合成管线是最长的管线，这进一步加剧了剖析错误：LLAMA会对最后的操作分配不足，除非使用技术来缓解错误剖析。我们对每个流水线使用50%流水线延迟目标。
#### 结论：
![table4](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/table4.40pjsc6rld60.png)

对于去噪流水线，禁用反馈和深度优先导致LLAMA对最后的均值移位操作分配的slack不足。对于合成流水线，当两种技术都关闭时，LLAMA虽然达到了延迟目标，但成本高出35%。在评估调用失败现象时，尽管认为造成了很高的失败率，但两条流水线都能达到指定的延迟目标。这些结果表明，深度优先和反馈对于解决执行过程中最开始的分析结果的差异是必要的，而且LLAMA对失败很稳健。

### 实验五 LLAMA做决策产生的开销
![table5](https://cdn.jsdelivr.net/gh/CAD2115/image-hosting@main/20211104/table5.z6r75pssjeo.png)

#### 结论：
计算slack值和确定配置的效率很高，因为推测只需要5微秒的时间。大部分时间是花在提交过程中评估操作之间的优先级，在调用过程中连接和发送调用到后端，以及在最终确定过程中调用完成后更新全局状态。

## 总结
对于文中的预测与提交队列中操作的具体调用流程比较模糊，通读论文后也是百思不得其解。目标函数的意义文中也讲的模糊，可能是由于对该方向知识的积累。在描述论文实验部分的写作方法时，老师提出好的实验分析不应该只是对功能有无的描述，最好能在功能相同之处进一步分析实现的具体差异而导致结果不一样的具体原因。
