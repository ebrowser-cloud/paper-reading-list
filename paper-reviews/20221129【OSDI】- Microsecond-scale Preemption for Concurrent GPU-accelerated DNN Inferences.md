# Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences

**[Microsecond-scale Preemption for Concurrent GPU-accelerated DNN Inferences](https://www.usenix.org/conference/osdi22/presentation/han)** 

## 论文作者：

* Mingcong Han：上海交通大学SEEIEE并行与分布式系统研究所，学生。
* Hanze Zhang：上海交通大学SEEIEE并行与分布式系统研究所，学生。
* Rong Chen：上海交通大学SEEIEE并行与分布式系统研究所，教授。
* Haibo Chen：上海交通大学SEEIEE并行与分布式系统研究所，教授。

## 解释：

* 幂等性：就是用户对于同一操作发起的一次请求或者多次请求的结果是一致的，不会因为多次点击而产生了副作用。

* 尽管 NVIDIA 声称他们的 GPU 自 Pascal 架构以来就配备了抢占支持，但没有公开可用的信息或软件可控接口。

## 背景：

一般的应用都会有两种类型的任务，real-time 任务：必须满足时间约束的；best-effort 任务：没有很强的时间约束。但是由于GPU缺乏高效的抢占调度支持，所以导致现有的解决方案一般都是：1. 为了减低延迟，让RT任务独占一整块GPU，但是这样使得GPU利用率下降。2. 为了提高吞吐量，让BE任务和RT任务以抢占式的方式共享一块GPU，但是由于GPU抢占式调度支持并不好，所以需要等待上一个任务执行完一个block，一般需要几毫秒，导致RT很长的时延，并且在RT任务以高请求率到达的情况下，会使得BE任务出现饥饿现象。

REEF 目标是：

* RT任务获得低延迟
* BE任务获得高吞吐量。

REEF 的一个关键见解是 DNN 推理中的每个内核大多是幂等的。

## 2 背景知识

### GPU 加速DNN推理

**DNN模型具有幂等性**：无论是否重试，内核总是可以用相同的输入产生相同的输出。

**DNN模型需要很多的kernel**：如Table 1所示

<img src="https://s3.uuu.ovh/imgs/2022/11/25/b725da848d5a70c3.png" alt="image-20221125195847785" style="zoom:50%;" />

**延迟具有可预测性**：一个DNN在单独的GPU上进行推理的时延是固定不变的。

**不同的并行性**：执行一个DNN推理时，有些层需要的CU（计算单元）比较少，可以和其他任务kernel并行；有些比较多，只能和少部分任务kernel并行。



### 现有最先进的 GPU 调度

**顺序执行**：clockwork 为代表的，没有抢占，所以会导致RT任务有较大的时延，没有并发执行，所以会导致整体吞吐量较差。如下图（a）

**block级抢占**：基于等待的方法来实现 GPU 调度的块级抢占。RT任务仍然需要等待正在运行的block完成。如下图b。此外，抢占延迟将随着抢占内核的数量而增加。高频RT任务会打断BE任务的执行，甚至导致饥饿。

**多个 GPU 流**：提供多个CUDA流来并发执行。虽然可以提高吞吐量，但是会增加很大的延迟开销，如下图c。

<img src="https://s3.uuu.ovh/imgs/2022/11/25/3c38cb312021e39e.png" alt="image-20221125200919077" style="zoom:50%;" />





## 3 REEF Overview

### 系统架构

<img src="https://s3.uuu.ovh/imgs/2022/11/25/c57f5d5fd82ec64d.png" alt="image-20221125203907852" style="zoom:50%;" />

#### DNN model preparation (offline)：

* code transformer module：验证DNN模型中kernel的幂等性，改造源码辅助GPU调度。
* kernel profiler：测量模型每个内核的计算要求和执行时间。

#### DNN inference serving (online)：

* Task Queues：一个RT que，一个 BE que。每个队列都绑定到用于启动 GPU 内核的 GPU 流。队列是FIFO。
* Scheduler：对任务队列的繁忙轮询并将任务分配给关联的 GPU 流。两种模式进行切换：
  * real-time mode：当RT队列不为空时。
  * normal mode：当RT队列为空时。
* Preemption module：
  * real-time mode：REEF 首先使用抢占模块立即从所有正在运行的BE任务中抢占 GPU，然后立即在 GPU 上启动实时任务。
  * normal mode：并发执行多个BE任务。
* Dynamic kernel padding (DKP)：
  * real-time mode：在启动RT内核之前，DKP 模块将选择合适的BE内核并将它们动态填充到RT内核中。



## 4 Reset-based Preemption

DNN 模型中的 GPU 内核大多是幂等的，这可以实现主动抢占——立即杀死 GPU 上所有正在运行的内核，并在稍后恢复它们。

优点：

* 不需要保存GPU运行环境，之后也不需要恢复。
* 不需要等待kernel运行结束。

挑战：除了正在GPU上运行的内核以外，GPU为了隐藏内核启动时间，数百个 启动内核 缓冲在由 GPU 运行时维护的多个队列中。但是很难在几百微妙内驱逐出所有的启动内核。



启动内核的生命周期分析：如下图

1. 调度器会启动推理任务的所有的内核，并且将这个推理任务制定一个GPU stream。
2. GPU 运行时 会为每一个GPU流缓冲启动内核，即维护一个链表，称为host queue。启动内核会放入这些队列中。
3. 每一个host queue都有一个后台进行，会将缓存的启动内核一步传送到一个环形缓存区，称为 device queue。该缓冲区CPU和GPU都可以访问。
4. GPU的Command Processor会轮询所有的device queue，并将其启动内核分派给计算单元。

综上可以看出来，启动内核在三个地方可能存在，分别是：HQ、DQ和CU。所以抢占的时候，需要驱逐这三个地方的启动内核。

<img src="https://s3.uuu.ovh/imgs/2022/11/25/3491517e50fc5e4f.png" alt="image-20221125210610267" style="zoom:50%;" />



### 4.1 Evicting Buffered Kernels

目的是从HQ和DQ中驱逐启动内核。

* 在HQ中，重置它们很简单，使所有缓冲内核出队并回收内存，因为它们完全由 GPU 运行时控制。

* 对于DQ，GPU运行时没有办法从DQ中删除启动内核，因为command processor会直接从DQ中拿启动内核，从而导致数据竞争和不可预测的结果。此外，CPU 也不提供从设备队列中安全驱逐内核的方法。

  解决方案：lazy eviction：

  * REEF的code transformer预先在每个kernel的开头注入一段代码，通过检查preemption flag来实现是否被evicted。

  * 当抢占标志为真时，内核将自行终止。

  * 因此，当发生抢占时，抢占模块会立即将 GPU 内存中的抢占标志设置为 true。缓冲在设备队列中的内核将像往常一样被提取并分派到 CU，但会立即自行终止。

这个机制会导致一个任务500微妙的抢占开销，经过深度分析，发现开销主要集中在：

* 从HQ中回收启动内核。
* 等待从DQ中拿走启动内核。

所以提出了两个优化来优化启动开销。

* Asynchronous memory reclamation：REEF 利用后台 GC 线程异步回收内存。具体来说，REEF通过简单的先清空头指针，然后通知GC线程在后台回收内存来重置主机队列。
* Device queue capacity restriction：lazy eviction：获取内核并将其分派到 CU，每个内核大约需要 20 微秒。调整DQ容量可在抢占延迟和执行时间之间进行权衡。随着队列容量的减少，RT任务抢占延迟也会减少，因为需要逐出的内核更少，但normal mode BE任务执行时间会增加，因为 GPU 有更多的空闲时间等待运行时用主机队列中的内核填充设备队列。

### 4.2 Killing Running Kernels

为了避免等待正在运行的内核完成，基于重置的抢占会主动杀死 GPU 中正在运行的内核。

但是现有的GPU既没有 GPU 运行时提供的 API，也没有 GPU 驱动程序公开的功能可以从主机端终止正在运行的内核。我们观察到 GPU 驱动程序有能力终止 CPU 进程并杀死相关的 GPU 内核，即使内核陷入无限循环。但是，此函数还将回收由进程和 GPU 内核分配的 GPU 内存。因此，被抢占的内核必须将 DNN 模型参数重新加载到 GPU 内存，甚至需要几秒钟。

为了解决这个问题，REEF 改造了 GPU 驱动的 kernel killing 功能，并将其暴露给 GPU runtime 中的抢占模块。新函数将指示命令处理器终止 CU 上所有正在运行的内核，但在 GPU 内存中保留它们的运行状态。在逐出主机队列和设备队列后，抢占模块将使用它来杀死所有正在运行的内核。

### 4.3 Restoring Preempted Tasks

被抢占的BE任务在RT任务执行完毕之后应该恢复运行。由于DNN模型的幂等性，所以只要恢复到被打断的内核的前面任意一个即可。为了资源应该尽量恢复到离被打断内核最近的内核。但是因为内核是直接被杀死的，所以很难准确预测被打断内核的位置。

为了解决这个问题，REEF 采用了一种近似方法来确保被抢占的任务从被中断的内核之前至多恒定数量 (c) 的内核中恢复。更具体地，抢占模块在开始重置任务队列时首先记录最后一个传输到设备队列的内核（kl），然后从kl之前的c个内核中恢复被抢占的任务，其中c表示设备队列容量。因为DQ的容量设置的比较小（eg 4），所以其恢复开销可忽略不计（30us）。



### 4.4 Preemption on closed-source GPUs

对于NVIDIA GPU是没有开源的，所以无法修改GPU runtime，主要限制是我们无法重置 CU 以主动终止正在运行的内核。除此之外，REEF 也无法直接在 GPU 运行时之外操作HQ和DQ。但幸运的是，REEF 提出的用于重置 DQ 的lazy eviction不需要对 GPU 运行时进行任何修改。

提出了受限版本的 REEF-N。

* 每个 GPU 流（GPU 运行时提供的一般抽象）包装到一个虚拟主机队列（vHQ）中，该队列拦截并缓冲所有启动的内核。
* 和物理HQ相似的是，也存在一个后台进行会异步的将缓存的内核传送到GPU runtime，相当于把GPU runtime当作了DQ。
* REEF-N接下来就要等待DQ内的内核 lazy eviction的形式被取走，同时**等待GPU上正在执行的内核执行完毕**。
* 为了限制DQ的容量，可以通过设置GPU runtime内未完成的kernel数来限制。



## 5 Dynamic Kernel Padding

受内核融合的启发，我们的方法还将RT内核和BE内核组合成一个内核，并使用单个 GPU 流启动它，如图 8 所示。不同的是，我们构建了一个模板（称为 dkp kernel）编译时并使用函数指针在运行时填充和执行内核。此外，我们动态选择BE的内核以避免干扰RT内核。如下图所示。

候选内核函数（例如，dense）不是静态内联到 dkp 内核中，而是被声明为单独的设备函数，它们可以作为 dkp 内核参数传递并由函数指针调用（第 3 行和第 8 行）。

dkp 内核对CU进行分区以并行执行一个RT候选内核 (rt_kern) 和一组BE的候选内核 (be_kerns)。它首先为RT内核分配足够的 CU（第 1-3 行），然后将剩余的 CU 分配给BE的内核（第 5-8 行）。启动RT内核时，DKP 模块会选择适当的BE内核与RT内核同时执行（第 10 行）。

<img src="https://s3.uuu.ovh/imgs/2022/11/25/d9bfcdd15a4147ca.png" alt="image-20221125225314614" style="zoom:50%;" />

 

### 5.1 Efficient Function Pointers

dkp kernel存在的两个性能问题：

* Limited register allocation：因为此时是在dkp kernel 上调用其他的内核，但是由于`禁止在 GPU 内核中直接使用函数指针` 这个机制所以可能会由于寄存器不足而迫使被调用者将变量保存在堆栈上，导致与纯粹使用寄存器相比性能较差。
* Expensive context saving：GPU 上的间接函数调用比 CPU 程序昂贵得多，因为在函数调用前后需要保存和恢复大量上下文（例如，几十个寄存器）。

解决方法：通过把kernel 函数声明为global function。由于全局函数被视为内核条目，编译器既不应用寄存器限制也不向它们添加上下文保存/恢复代码。

但是即使用 global function 也会出现很多问题。

* Dynamic register allocation：为了满足候选内核的不同寄存器需求，dkp 内核必须分配尽可能多的寄存器（即过度分配），这可能会减少 CU occupancy，从而增加执行时间。一个直观的解决方案是在 dkp 内核启动之前及时覆盖寄存器计数，使其适应选定的候选内核。不幸的是，在离线阶段，dpk kernel 的寄存器计数已经加载到 GPU 内存中，这意味着覆盖它的值需要在每次内核执行之前将 CPU 复制到 GPU 内存，严重影响执行性能。代理内核与图 9 中的 dkp 内核共享相同的源代码，但分配不同数量的寄存器，允许调度程序根据每个候选内核的寄存器需求动态选择合适的代理内核。不幸的是，为每个可能的寄存器计数生成代理内核面临着内核数量爆炸的问题。例如，在每个线程最多有 128 个标量寄存器和 256 个向量寄存器的 AMD Instinct MI50 GPU 上，它将生成 32,768 个代理内核以覆盖所有可能的寄存器配置。为了减少代理内核数量，我们生成代理内核以覆盖所有可能的 CU 占用而不是寄存器计数。由于引入代理内核是为了防止过度分配导致CU占用减少，因此具有不同寄存器计数但共享相同CU占用的代理内核实际上是冗余的，可以合并在一起。更具体地说，我们使用的 AMD Instinct MI50 GPU 上有 10 个 CU 占用级别，对应于 10 个寄存器计数范围，这允许我们只生成 10 个代理内核，每个代理内核分配一个 CU 占用级别允许的最大数量的寄存器。对于每个候选内核，调度程序选择具有最少分配寄存器的代理内核来满足候选内核的需求，从而实现最高的 CU 占用率。
* Dynamic shared memory：共享内存的过度分配也可能会降低代理内核的 CU 占用率。幸运的是，内核可以在启动内核时通过设置一个属性（即“动态共享内存”）来动态分配共享内存。

### 5.2 Kernel Selection

为RT任务选择合适的BE任务。

* 规则 1. 尽力而为内核的执行时间必须比实时内核的执行时间短。
* 规则2. best-effort kernel的CU占用率必须高于real-time kernel



## 7 Evaluation

### 7.1 Experimental Setup

Testbed：实验主要在 GPU 服务器上进行，该服务器由一个 Intel Core i7-10700 CPU（共 8 个内核）、16 GB DRAM 和一个 AMD Radeon Instinct MI50 GPU（60 个 CU 和 16GB 内存）组成。服务器软件环境配置为ROCm 4.3.0 [3]、Apache TVM [73] 0.8.0、Ubuntu 18.04。我们在闭源 GPU（NVIDIA V100 GPU）上进一步评估 REEF-N，以证明我们方法的通用性，使用安装了 CUDA 10.2 [52] 的同一台服务器。

Workloads：

<img src="https://s3.uuu.ovh/imgs/2022/11/26/bac601889864bcd0.png" alt="image-20221126095007003" style="zoom:50%;" />

比较策略：

* SEQ：顺序的执行DNN推理任务，具体来说，当队列中有多个任务等待时，它会优先考虑RT任务，但仍然需要等待已启动的BE任务完成。RT任务一个一个执行，BE任务并发执行。
* GPUStreams：通过多个 GPU 流在同一个 GPU 上同时运行RT和BE任务，
* RT-Only：将 GPU 专用于实时任务。



### 7.2 Overall Performance

比较了在DISB和真实负载情况下，四个策略的性能，包括：吞吐量和时延。

<img src="https://s3.uuu.ovh/imgs/2022/11/26/c6e51e4b62b5e555.png" alt="image-20221126100744087" style="zoom:50%;" />

上图是RT任务的时延，下图是BE和RT任务的总吞吐量。

结果分析：

* 单个BE client【DISB A、DISB B】：对于具有单个 BE 客户端的工作负载，使用 SEQ 或 GPUStreams 对性能的影响相对较低，因为BE任务的 GPU 争用并不严重，无论是在等待时间 (SEQ) 还是并发干扰 (GPUStreams) 方面。
  * DISB A：和RT-only相比
    * SEQ 和 GPUStreams 将整体吞吐量提高了 1.46 倍和 1.66 倍，但也将RT任务延迟分别放大了 1.95 倍和 1.84 倍；
    * REEF 在实时任务延迟方面的开销可以忽略不计 (0.5%)，整体吞吐量提高了 1.60 倍。
  * DISB B：由于更频繁地运行RT任务
    * SEQ 的RT任务延迟降低了 1.12 倍，略好于 DISB A，因为它只需要等待更少的BE的任务，然而，它的吞吐量只能达到 RT-Only 的 96%，因为RT任务会使 GPU 饱和，而BE的任务几乎没有机会运行；
    * 出于类似的原因，GPUStreams 的整体吞吐量也下降到 RT-Only 的 76%，而其RT任务延迟仍然比 RT-only 高 1.70 倍；
    * REEF 仍然可以将RT任务延迟的开销限制在 1%（约 60 μs），并在整体吞吐量上提供 1.14 倍的加速。
* 多个BE clients【DISB C、DISB D、DISB E】：随着BE工作负载的增加，通过在两种类型的任务之间共享 GPU，所有方法的总体吞吐量都比 RT-Only 有不同程度的提高。
  * SEQ 和 GPUStream 都在实时任务延迟和整体吞吐量之间做出相似的权衡。对于三种工作负载，SEQ 将整体吞吐量提高了 1.34 倍至 2.10 倍，但也将实时任务延迟放大了 1.51 倍至 1.86 倍。对于 GPUStreams，上述数字变为 3.94× 至 8.19× 和 2.65× 至 3.31×。
  * 时延：REEF 在所有工作负载中提供与 RT-Only 几乎相同的实时任务延迟，开销不到 1.5%（0.1 毫秒）。吞吐量：REEF 在 DISB C 上提供了 GPUStreams 的接近结果，因为 VGG 很容易被大多数 DNN 模型填充。在 DISB D 和 E 上，REEF 的吞吐量比 GPUStreams 低约 25%，这是由于混合使用了五个 DNN 模型进行实时任务，而 DKP并不能很好的将RT任务和BE任务组合在一起。然而，REEF 仍然分别优于 RT-Only 3.00 倍和 2.96 倍。
* 真实负载【REAL】
  * 与 RT-Only 相比，SEQ 和 GPUStreams 将整体吞吐量提高了 3.6 倍和 8.3 倍，同时将实时任务的延迟分别放大了 1.35 倍和 3.35 倍。
  * 由于真实世界轨道中实时任务的负载较低（约 43 reqs/s），REEF 与 GPUStreams 类似，大部分时间保持在normal mode并发执行BE的任务。因此，与 RT-Only 相比，REEF 实现了 7.7 倍的吞吐量提升，实时任务的延迟开销不到 2%，这要归功于我们基于重置的抢占，它可以在实时任务到达后的几十微秒内抢占 GPU。



### 7.3 DNN Inference Preemption

baseline：基于等待的抢占，在其上面实现了并发和lazy eviction。

**Preemption latency**

<img src="https://s3.uuu.ovh/imgs/2022/12/02/f70d4c1ad1b14d87.png" alt="image-20221126192500761" style="zoom:50%;" />

实验结果：对于所有 DISB 工作负载，基于重置的抢占优于基于等待的方法超过一个数量级，从 15.3× (DISB E) 到 18.5× (DISB C)。

出现这样实验结果的主要原因是：基于等待的方法需要等待正在CU上的内核执行完成，并且在HQ和DQ中存在大量的启动内核需要驱逐。而reset-based则有相应的优化措施。

**测试对于不同DNN模型的抢占时延：**

我们使用单个 BE 客户端发送给定模型的推理请求，并在随机时间间隔后发送实时请求以抢占 GPU。

如图 12(b) 所示，由于内核数量和执行时间的差异，基于等待的抢占延迟高度依赖于模型类型，从 268 μs (VGG) 到 790 μs (RNET) （见表 1）。相比之下，基于重置的方法对 DNN 模型不敏感，并且可以在所有五个模型的 35 μs 到 38 μs 范围内抢占 GPU。

<img src="https://s3.uuu.ovh/imgs/2022/11/26/d6dfd50455bd9497.png" alt="image-20221126193946544" style="zoom:50%;" />

**不同模型属性对抢占延迟的影响**

为了进一步研究不同模型属性对抢占延迟的影响，我们模拟了具有不同启动内核数和内核执行时间的 DNN 模型。

默认情况下，我们将启动内核的数量和内核执行时间分别设置为 100 和 100 μs。

<img src="https://s3.uuu.ovh/imgs/2022/11/26/eca3044c553d7d6f.png" alt="image-20221126194147071" style="zoom:50%;" />

如图 13 所示，基于等待的方法的抢占延迟线性上升，而我们的基于重置的抢占方法在非常低的延迟（小于 40 μs）下保持稳定。对于基于等待的方法，抢占延迟与启动内核的数量和内核执行时间显着正相关，因为它必须等待启动内核的驱逐和运行内核的完成。相比之下，基于重置的方法会主动重置 GPU 运行时中的主机和设备队列以及 CU，其中成本与模型属性无关。

**检验提出的两种优化方法的性能：异步内存回收和队列容量限制。** 

<img src="https://s3.uuu.ovh/imgs/2022/11/27/a9105ad80b7ea796.png" alt="image-20221127093812108" style="zoom:50%;" />

如图 14(a) 所示：

* 通过启用两项优化，抢占延迟显着下降从87% 到 92%。
* 没有启动两项优化，基于重置的方法仍然比基于等待的方法高达3倍。

如图14(b)所示：对于单个 BE 客户端

* 使用异步内存回收将重置HQ的延迟从 17 微秒减少到 3 微秒。
* 同时，使用队列容量限制进一步将重置DQ的延迟从 424 微秒减少到 31 微秒。

**Queue capacity**

<img src="https://s3.uuu.ovh/imgs/2022/11/27/145a227f58e92a5d.png" alt="image-20221127100239817" style="zoom:50%;" />

从图15可以看到：

* 减少DQ容量会增加推理的执行时间，这个是因为GPU有了更长的空闲时间来等待运行时从HQ中拿取内核填满DQ。所以应该增加DQ容量来减少推理的执行时间，但是当DQ容量大于4，其推理的执行时间的减少几乎没有。
* 减少DQ容量会提高CPU利用率，同时可以缩短抢占时延。所以应该减少SQ的容量，但是当DQ容量小于4，其推理时延的缩短也几乎没有。

综上，最后选择的DQ容量为4！

**Task restore** 

进一步评估由于任务恢复，其抢占任务的执行开销。

<img src="https://s3.uuu.ovh/imgs/2022/11/27/eb3c99ca4807f6dd.png" alt="image-20221127102107489" style="zoom:50%;" />

从图15 b可以看出来，任务的恢复开销非常的短，从70us到225us，这取决于kernel的执行时间，如图10所示。因为DQ容量设置为4，所以最多重复执行5个内核。此外，所有 DNN 模型的执行时间开销约为 2%，但 VGG (5.1%) 除外，因为它具有最少的内核 (55)，并且其内核执行时间更长。

### 7.4 Dynamic Kernel Padding

我们使用高竞争工作负载，其中一个 RT 客户端和一个 BE 客户端同时以足够高的频率发送请求以保持 GPU 忙碌。

**Performance**

<img src="https://s3.uuu.ovh/imgs/2022/11/27/e62380fab362d02e.png" alt="image-20221127102746199" style="zoom:50%;" />

<img src="https://s3.uuu.ovh/imgs/2022/11/27/3615a428afef0cf2.png" alt="image-20221127103406035" style="zoom:50%;" />

结果分析：

* 时延分析：

  * GPUStreams 将实时任务延迟显着放大了 1.35 倍，范围从 1.04 倍到 1.70 倍不等。

  * 然而，REEF 能够为实时任务提供几乎最佳的延迟，平均开销仅为 1%（最高 3%）。

* 吞吐量分析：

  * 尽管 GPUStreams 将整体吞吐量平均提高了 1.52 倍，但由于并发执行的严重干扰，实时任务的吞吐量平均下降了 24.4%。
  * 相反，REEF 首先保证实时任务的吞吐量，然后利用动态内核填充来提高整体吞吐量。性能提升主要取决于两个条件。
    * 首先，在 GPU 上执行实时任务还有待改进。如图17所示，RT任务是IN3或者BERT时，其吞吐量提升就不太明显。
    * 其次，BE内核的执行时间必须比填充RT内核的执行时间短。这就可以解释为什么用RNET来填充VGG有很大性能提升，反之则不行。

**Optimizations**

为了研究优化对性能和内存使用的影响，我们首先使用 GPU 上函数指针的不同实现来评估开销。

我们通过 dkp 内核启动RT内核而不填充任何BE的内核来测量这种开销。

<img src="https://s3.uuu.ovh/imgs/2022/11/27/f188c6ebf5dd7af8.png" alt="image-20221127103900319" style="zoom:50%;" />

结果分析：

* 时间开销，图18 a
  * 对于具有不同 DNN 模型的RT任务，默认函数指针实现（默认）会导致执行时间开销从 78% 到 503% 不等。
  * 通过使用全局函数指针（GlobalPtr），平均开销显着降低到 46.4%（从 11.5% 到 120%），因为它消除了对设备函数指针寄存器数量的限制，避免了在函数调用期间额外寄存器的保存和恢复。 
  * 最后，通过使用代理内核（ProxyKernel），开销平均下降到 0.8%（最多 1.21%），它可以为每个内核动态分配寄存器并最大化 CU 占用。
* 内存使用，图18 b
  * 如图 18(b) 所示，使用静态核融合（Kernel Fusion）需要超过 35 GB 的 GPU 内存来存储五个 DNN 模型的融合核——所有组合不超过三个核，这甚至超过了内存容量大多数商品 GPU。
  * REEF 提出了代理内核（DKP w/o OPT）来将 GPU 内存使用量减少到大约 32 MB。
  * 最后，生成代理内核以覆盖所有可能的 CU 占用（DKP w/ OPT），而不是所有可能的寄存器配置，可以将 GPU 内存使用量显着减少到仅 10 KB。

**Kernel selection**

测试了动态内核填充期间 DISB A-E 内核选择的平均时间。

<img src="https://s3.uuu.ovh/imgs/2022/11/27/5f358b1acd26c3f4.png" alt="image-20221127105609912" style="zoom:50%;" />

结果分析：

* 对于具有单个 BE 客户端（DISB A 和 B）的工作负载，REEF 大约需要 0.2 微秒来为给定的实时内核选择尽力而为的内核。
* 由于候选者较多，对于具有多个 BE 客户端（DISB C、D 和 E）的工作负载，选择时间增加到 0.4 μs。一般来说，内核选择的成本是微不足道的，很容易被内核执行所隐藏。

为了进一步研究内核选择的准确性，我们评估了由于在所有 DISB 工作负载上填充BE内核而导致的RT内核的执行时间开销。

* 如图 19(b) 所示，超过 37% 的实时内核没有受到与尽力而为内核并发执行的负面影响，并且超过 90% 的实时内核的开销仍然小于 4 微秒。执行时间的增加主要是由于对 GPU 内存和共享二级缓存的争用。



### 7.5 Closed-source GPUs

比较了REEF-N在AMD和NVIDIA上的性能，以及基于等待的在NVIDIA上的性能和REEF在AMD上的性能。

<img src="https://s3.uuu.ovh/imgs/2022/11/27/8c54fd796a352700.png" alt="image-20221127105928259" style="zoom:50%;" />

结果分析：

* 虽然REEF-N不能主动的终止正在运行的内核，但其抢占延迟的范围仅为 71μs 至 288μs，在 NVIDIA GPU 上仍然比基于等待的方法高出 12.3 倍（从 6.3 倍）。
* 通过比较 AMD GPU 上的 REEF-N 和 REEF，我们观察到主动终止正在运行的内核进一步有助于平均加速 2.0 倍的抢占延迟，特别是对于抢占并发任务（例如，DISB C 为 2.3 倍）。
* 此外，REEF-N 在两个 GPU 上的性能接近。



## 9 related work

* DNN inference serving systems

  * Clockwork
  * Clipper、Nexus
  * Abacus
  * INFaaS

  然而，数据中心应用程序的延迟 SLO 比实时系统的延迟 SLO 宽松得多，例如其单运行延迟的 2 倍 [22]。因此，使用非抢占式调度或批处理方案对数据中心应用程序有效，但对实时场景（例如自动驾驶汽车）无效。此外，REEF 的设计与上述分布式服务系统是正交的。 REEF 中的两个关键机制也可以集成到其中，以提高每个 GPU 的吞吐量并保持实时推理的低延迟。

* GPU kernel preemption：支持 GPU 上的上下文切换

  * 轻量级上下文切换以避免不必要的寄存器保存  [44]
  * 通过停止发布新线程块，扩展硬件以被动抢占 GPU 的流式多处理器 (SM)。[77]
  * Chimera [56] 进一步提出 SM 刷新以在检测到幂等执行时立即抢占 SM。

  不同的是，我们的方法改造了现有的硬件机制，不需要修改 GPU 来实现即时抢占。

* GPU multitasking

  * 对于 DNN 计算，Rammer [47] 采用整体方法在编译时利用内核间和内核内并行性，它使用静态内核融合 [74] 来强制执行并发内核的 CU 分配。
  * 之前的工作还提出了建模和预测并发内核执行速度减慢的方法 [13、14、86、88]。预测可以帮助做出调度决策以匹配实时内核的延迟要求。



## 总结

本文主要是在单个GPU上为RT和BE任务提供相应的服务满足这两种请求各自的目标。对于RT任务是尽可能减少其响应时间，对于BE是提高吞吐量。本文主要是提出了基于重置的抢占以及动态的为RT任务添加BE任务。通过前者使得RT任务可以迅速抢占GPU，微秒级别，从而减少了RT任务的响应时间。通过后者使BE任务可以填充到RT任务并发执行且不影响其性能，从而提高了GPU的吞吐量。本文通过修改开源GPU AMD来实现上述功能，对于闭源GPU NVIDIA，本文也实现了REEF-N，基本实现了上述功能，但是由于无法直接杀死正在运行的kernel，所以使得抢占开销更大。

本文由于使用BE任务去填充RT任务是有条件的，比如：需要BE任务中kernel执行时间短于当前执行的RT任务的kernel。所以在当前BE kernel集合中没有比当前执行的RT任务kernel更短的执行时间时，只能运行RT任务的kernel，也浪费了GPU资源，达不到提高吞吐量的目标。









