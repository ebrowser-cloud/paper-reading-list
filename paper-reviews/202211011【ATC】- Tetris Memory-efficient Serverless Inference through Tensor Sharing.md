# Tetris: 通过张量共享实现内存高效的无服务器推理

**[Tetris: Memory-efficient Serverless Inference through Tensor Sharing](https://www.usenix.org/conference/atc22/presentation/li-jie)**  

首先发现了无服务器内存消耗过多的问题，通过实验发现主要是因为运行时冗余、模型张量冗余以及缓存冗余导致的。本文主要从模型张量冗余入手，辅助运行时冗余来提高内存利用效率，节约无服务器推理成本。



## 研究动机

### 发现一：内存密集型启动：无服务器模型推理过程中，加载模型占大部分时间。

<img src="https://s3.bmp.ovh/imgs/2022/10/22/081191b27f0858c2.png" alt="image-20221022194636071" style="zoom:50%;" />

DL推理函数在启动时需要加载大量的模型参数，直到模型参数全部从磁盘中读取并填充到计算图中，才会进行模型推理。启动页面缓存来加速启动的前提下，测了请求处理的三个阶段，分别是：sandbox and runtime initialization, model loading, and inference computation。从图中可以看到加载模型占大部分时间。



### 发现二：内存密集型计算：推理计算需要大量内存，模型参数消耗最多。

推理过程的内存占用可分为以下部分：program code, libraries, the model,the function call stack, intermediate tensors and the network buffers.

<img src="https://s3.bmp.ovh/imgs/2022/10/22/5f3b973748062516.png" alt="image-20221022195422870" style="zoom:50%;" />

通过图b可以看到：在推理过程中，model本身消耗的内存最多。



### 发现三：运行时冗余

由于多次启动的实例，推理运行时在内存中被复制。虽然并发执行和批处理都支持运行时共享，但如何将它们结合使用具有挑战性。多次启动的实例会导致重复的运行时内存消耗。可以考虑将多个请求打包到一个实例中进行推理，从而减少内存的占用。

* 实验一：运行时共享两种方案：batching 和 concurrent execution

  <img src="https://s3.bmp.ovh/imgs/2022/10/22/379048aa0109bda3.png" alt="image-20221022214631435" style="zoom:50%;" />

  可以看到，增加并发数量或者增加batch size都会显著的增加推理延迟。前者是因为对计算资源的争用，后者是因为增加了计算负载。

* 实验二：进一步研究了在batching 和 concurrency 的各种组合下的内存消耗

  <img src="https://s3.bmp.ovh/imgs/2022/10/22/3496c298cd8ca80b.png" alt="image-20221022215600488" style="zoom:50%;" />

  增加batch会容易违反SLO，增加concurrency可能会导致消耗更多的内存。所以如何选择一个合适的组合是非常重要的。



### 发现四：张量冗余：常数的张量，模型参数，在函数实例中被广泛复制

张量冗余的两个情景：

* 不同管道因业务而异，但是他们使用的相同的模型的张量保持一致：
  * 比如：光学字符识别 (OCR) 流水线和图像审核流水线共享相同的文本行识别和识别模型（ResNet），差异是：图像审核流水线包含一个额外的关键字检测模型。
  * 有些模型有特定的预处理和后处理模块，用于处理不同格式的数据，从而生成多版本模型管道。
* 预训练和迁移学习：
  * 为一项任务训练具有大量数据集的模型，其中学习的参数可以在其他相关任务中重复使用。而不需要全部重新开始训练。
  * 很多具有相似任务的模型都是基于预训练的模型构建的。不是对模型的所有参数进行微调，而是直接重用部分参数并仅微调一小组模型层。



### 发现五：缓存冗余

无服务器平台和 DL 库中内存缓存的广泛使用加剧了运行时和张量冗余。

* serveless为了缓解冷启动的开销，会在推理执行完毕之后keep function alive and warm，该缓存加剧了运行时和张量冗余问题。
* TensorFlow 利用 MKL-DNN等高性能计算库来加速计算图的执行。在执行卷积等操作时，TensorFlow 的内部张量表示与 MKL-DNN 不完全相同；因此，必须将 TensorFlow 格式的卷积核数据转换为 MKL-DNN 格式。这些转换后的参数通常缓存在内存中以供后续使用。



### 总结：

发现一和发现二：无服务器推理过程中，计算消耗大部分内存，启动占用了大部分时间，这个主要是由于模型参数加载消耗大部分时间，如果说之前已经加载到内存中的张量可以不用重复加载，则可以加快启动，同时也减少张量冗余，节约内存资源。

发现三、发现四和发现五：冗余主要来自三个方面，分别是：运行时冗余、张量冗余和缓存冗余。所以可以考虑从这三个方面入手来提高内存使用效率。





## 系统设计

### 系统总览

Tetris设计思路是通过tensor sharing和runtime sharing组合优化来提高内存效率。

<img src="https://s3.bmp.ovh/imgs/2022/10/23/0181ad67d7a0e16f.png" alt="image-20221023110245765" style="zoom:50%;" />



* developer 提交模型之后，TETRIS会提取张量信息，并且配置在不同的batch和concurrency的情况下来分析推理延迟。配置信息用于scaling and scheduling模块的来决定运行时选择的batch和concurrency。
* user提交请求时，scaling and scheduling模块根据现有工作负载情况决定扩展新实例还是重用现有实例。
  * 扩展：根据配置信息，选择合适的batch和concurrency，在不违背SLO的情况下，最小化内存的占用。并且将该container放置到最大tensor相似度的server上。
    * TETRIS 支持通过每个沙箱中的特殊代理首次共享张量。
    * 代理解析模型的计算图并读取张量的哈希值。
    * 然后，代理检查张量存储是否已经存储了它。
    * 如果是，则agent只使用Mmap的syscall将张量的内存地址映射到其本地进程，张量的引用数加1。
    * 否则，代理直接将模型文件中的张量加载到内存中，在张量存储中创建一个新项目。
  * 在复用已有实例的情况下，按照负载均衡的原则将请求转发给实例。
* 当工作负载减少时，扩展和调度引擎还会选择性地释放负载最少的实例，而不会违反 SLO。释放后，其张量的参考数减少 1。
* 在每台服务器中，都有一个内存回收器定期验证每个张量的引用号，并回收reference = 0的内存页面，以便释放的函数不再占用内存。
* 可以采用keep-alive策略来避免回收振荡。



### Scaling and Scheduling (Runtime Sharing)

依赖于模型配置文件和每秒请求数 (RPS) 来做出扩展决策。

* Profiling：测量各种配置下每个模型的推理延迟
  * 因为在CPU上进行推理，而 CPU 上的深度学习推理通常使用小批量（例如，B = {1, 2, 4}）和低并发（例如，P = {1, 2, 3, 4}）以实现低延迟。所以profile的过程只考虑这些组合。
  * 在分析阶段，TETRIS 还使用循环冗余校验 (CRC) 代码计算和存储张量的哈希值，用于在张量共享期间检查内存状态。
  * 分析之后，为每一个模型生成五元组： $(c, m, b, p, l)$  ，c 表示分配的 CPU 数量，m表示内存配置，b表示最大批量大小，p表示并发推理线程数，l 表示先前配置下的推理延迟。
* Runtime-shared scaling：scaling engine 模块监控实时 RPS 并判断现有实例是否足以满足这些请求。
  * 如果不满足，它会将部分请求分派到现有实例并启动新实例以处理剩余实例。给定模型配置文件和剩余 RPS（用 R 表示），扩展引擎探索新实例的最佳配置，以最大限度地减少内存使用量，同时保证它们的延迟 SLO。
  * 算法：Decreased Throughput Selection
    * 按照标准化吞吐量进行降序排序，$(b_i*p_i)/(l_i*m_i)$ 
    * 贪心的选择更大的标准化吞吐量的配置。
* Scheduling：一旦扩展引擎确定了新实例的配置，调度程序就会将它们部署到适当的服务器上。
  * 为了通过张量共享减少集群级别的内存消耗，TETRIS 总是尝试将它们分派到具有最大张量相似度（用 θ 表示）的服务器上。
  * TETRIS 首先过滤掉不满足资源要求（例如 CPU 和内存）的服务器。
  * 使用 $θ_{ij} = Mem(T_i ∩ T^j_{store})/Mem(T_i) $ ， $T_i$ 表示实例 i 中的张量集合，$T^j_{store}$ 表示已经存储在服务器 j 中的张量， $Mem(T_i)$ 表示 $T_i$ 中张量的聚合内存。



### Tensor Sharing

启动步骤

* 启动沙箱
* 沙箱中激活代理以将模型加载到主内存中
* 对于已加载到张量存储中，代理将其内存地址映射到实例。
* 否则，它会在张量存储中为其创建一个新项目。

优化：

* Agent：

  * 每当将模型加载到内存中时，代理都会读取存储在模型文件中的计算图元数据并解析需要加载到内存中的张量。

* Tensor store：

  * 通用张量：包含需要在服务器上的所有函数实例之间共享的所有张量内存。
    * 服务器上的每个函数实例都可以访问张量存储中的张量。
    * 每个张量都由一个哈希值唯一标识，该哈希值是根据张量的内容和维度计算得出的。
    * 每个张量都与一个在创建后用 1 初始化的参考号相关联。
    * 每当代理向现有张量添加新映射时，引用数增加1。
    * 同样，每当完成后释放实例时，引用数减少1。
    * 将引用数设置为后回收张量内存0。
  * 专用张量：TETRIS 还支持为本地服务器上的函数子集（例如，属于同一租户的函数）构建专用张量存储。未经许可的函数不能访问专用张量存储中的张量。



### Memory Reclaiming

* 内存回收器，它会定期检测并回收引用数 = 0 的张量的内存。

* 回收器还支持张量缓存策略，即使张量的引用数变为0，也将张量保留在张量存储中。
* 回收器的两种张量缓存策略
  * keep-alive window：这是一个超时阈值，用于确定张量保持活动多长时间；【按照时间来删除】
  * Least Recent Used (LRU)：它仅在张量存储已满后将最近或经常使用的张量保留在张量存储中。【给回收器设置一个存储空间】

<img src="https://s3.bmp.ovh/imgs/2022/10/23/74e681ba51747eca.png" alt="image-20221023152617940" style="zoom:50%;" />



## 实验评估：

### 实验环境：

#### Testbed:

2台机器80核256GB内存，6台32核128GB内存。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/4c69dd5751e8ad3e.png" alt="image-20221025095122467" style="zoom:50%;" />

#### Workloads:

21 个推理模型：

<img src="https://s3.bmp.ovh/imgs/2022/10/25/5c3d6eef4ab112df.png" alt="image-20221025095217423" style="zoom:50%;" />

四个现实生活中的应用：

* 二手车交易 (SVT) 应用程序
* 音频问答 (QA-Audio) 应用程序
* 语义相似度计算 (SS) 应用程序
* 文本问答 (QA-Text) 应用程序

三种典型的生产跟踪类型：

* 稳定的、周期性的和突发的。

  <img src="https://s3.bmp.ovh/imgs/2022/10/25/65ef16d6f998d8d8.png" alt="image-20221025095341431" style="zoom:50%;" />

#### Competing approaches:

* Tetris

* INFless：批处理来支持运行时共享
* Photons：并发执行来支持运行时共享
* Tetris-RO：仅运行时共享



### 验证张量共享的效果：

#### 实验一：推理函数实例内部的张量共享

运行**相同模型**的实例：

* 在推理函数的实例内部共享张量可节省高达 93% 的内存。

  将instance使用的内存分成两个部分： $M_{tensor}$  张量共享的内存占用，  $M_{others}$ 实例间不能被共享的内存占用。

  tensor 共享：$M_{ts} = M_{tensor}+I*M_{others}$ 

  无tensor共享：$M_{baseline} = I*(M_{tensor}+M_{others})$ 

  <img src="https://s3.bmp.ovh/imgs/2022/10/25/071d52e4bcfccf21.png" alt="image-20221025095820461" style="zoom:50%;" />

  分析发现：

  * 实例个数越多，内存减少的个数越多。
  * $M_{tensor}$ 越大，内存减少的越多。

* 加快了启动速度：来自同一函数先前实例的张量共享可以显着加快启动过程。【该模型实例已经加载过】

  <img src="https://s3.bmp.ovh/imgs/2022/10/25/52c4fff0f99d9e5b.png" alt="image-20221025102109344" style="zoom:50%;" />



#### 实验二：推理函数之间的张量共享

**不同模型**，但是存在**相同层**的模型变体。

* 跨函数共享张量可以进一步减少高达 36.3% 的内存。

  增加模型的变体数量，观察模型的系统内存的减少率。

  <img src="https://s3.bmp.ovh/imgs/2022/10/25/ff4557e0baac43b7.png" alt="image-20221025101120461" style="zoom:50%;" />

  观察发现：

  当模型的变体数量增加，模型占用的内存会进一步减少。

* 跨函数共享张量也可以加速启动。【该模型的变体加载过】

  <img src="https://s3.bmp.ovh/imgs/2022/10/25/05e1d42b12438cff.png" alt="image-20221025102345405" style="zoom:50%;" />



### 整体评估：

评价对象：SVT, QA-Audio, SS and QA-Text

#### 实验一：Tetris 减少了内存消耗。

TETRIS 在内存消耗方面明显优于 INFless 和 Photons。在不同请求率到达情况下，四种app的内存消耗情况。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/3126ef7dd1960768.png" alt="image-20221025102822017" style="zoom:50%;" />

观察发现：

* INFless 消耗的内存最多。
  * INFless 只有在模型支持批处理的情况下才能减少内存消耗。但是，对于应用程序 QA-Audio 和 SS，有些模型（即 Fastspeech2 和 Use）不支持批处理。
  * 批处理引入了额外的排队时间。虽然 CPU 上的推理计算很慢且 SLO 很紧，但即使是启用批处理的模型，我们也无法配置更大的批处理大小。
  * INFless 更倾向于使用跨越多个服务器的碎片化资源，以更好地容纳剩余负载并提高资源利用率，这加剧了内存消耗。
* Photons 消耗的内存比INFless 少。
  * 因为它在同一个实例中同时执行请求而没有批处理排队时间。
* 没有tensor sharing的Tetris表现的比前面两者都好。
* 存在tensor sharing的Tetris更大程度的减少了内存的消耗，表现最好。

#### 实验二：运行时共享的优化。

高效的运行时共享：TETRIS 中批处理和并发执行的组合优化优于 INFless 的批处理或 Photons 的并发执行。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/f604637e93d6c3d1.png" alt="image-20221025105153420" style="zoom:50%;" />

从Fig 13可以看到，在SVT, QA-Audio, QA-Text这三个app下，只有运行时共享的Tetris-RO比INFless和Photons消耗的内存更少。这是因为TETRIS 中批处理和并发执行的组合优化优于 INFless 的批处理或 Photons 的并发执行。虽然在SS这个app下，Tetris-RO虽然消耗的比Photons多，但是从Fig12可以看出来，Photons需要消耗更多的CPU资源。

#### 实验三：Tetris 对推理工作负载的延迟SLO的保证。

SLO 保证：TETRIS 可以保证推理工作负载的延迟 SLO。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/0d306fe6fc3b2d69.png" alt="image-20221025105647694" style="zoom:50%;" />

TETRIS 和 INFless 都可以获得一个很低的违规率【小于 4%】，虽然有些情况下，Tetris比INFless略高，但是TETRIS 以微不足道的 SLO 违规率增加为代价，显着提高了内存效率。

#### 实验四：TETRIS 减少内存消耗为推理服务提供商节省了可观的金钱成本。

节省成本：TETRIS 减少内存消耗为推理服务提供商节省了可观的金钱成本。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/1e88f30eb653afe4.png" alt="image-20221025110231916" style="zoom:50%;" />

如图所示：Tetris与INFless和Photons相比，每100个请求分别减少了61.5GB 和 20.2GB的内存。如果按照 AWS EC2 上的 r6g.medium 服务遵循每小时 0.0504 美元的定价模型，这种内存减少可以转化为每 100 个请求节省 0.000861 美元和 0.000283 美元的成本。



### 开销评估：

#### 实验一：Profiling 的开销。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/45e85c48a156a538.png" alt="image-20221025110729108" style="zoom:50%;" />

分析推理模型平均需要 12 分钟。

#### 实验二：验证TETRIS 中的内存地址映射方法不会引入延迟开销。

与直接在 TensorFlow Serving 框架上部署所产生的延迟相比，使用 TETRIS 后没有观察到性能下降。

<img src="https://s3.bmp.ovh/imgs/2022/10/25/a952989ffd83e101.png" alt="image-20221025110910715" style="zoom:50%;" />

#### 实验三：当同时创建多个实例时，张量锁的争用可能会导致启动开销。

分析：通过在张量加载过程中采用随机化，TETRIS 仍然优于基线，从而减少了很多锁争用。一旦张量成功加载到内存中，TETRIS 只需要为新启动的实例映射内存地址，而基线仍然需要昂贵的文件读取和解码。

<img src="https://s3.bmp.ovh/imgs/2022/11/07/852aec84efb87ece.png" style="zoom:50%;" />





## 相关工作：

![image-20221107115457380](https://s3.bmp.ovh/imgs/2022/11/07/9b9d6712cc0a64f7.png)



## 其他：

### 疑问：

* Agent 是否加载模型到内存？

  应该是没有加载的，因为有模型的Profile部分，所以模型在这个时候已经被拆分成多个张量，并且以计算图元数据的形式进行存储。所以之后用户提交模型，代理都会读取存储在模型文件中的计算图元数据并解析需要加载到内存中的张量。【基于猜测，不是非常确定】

* 现有的商业平台的无服务器推理是一个请求启动一个容器吗？



### 未来工作：

这篇文章主要是在无服务器的CPU上进行推理，可以考虑扩展到GPU。



### 总结：

本文的创新点在于通过实现张量共享来优化内存：

* 模型配置。分析模型，生成模型计算图元数据，以及计算出不同batch和concurrency情况下，模型的执行时间。
* 请求调度。将收到的请求根据配置文件，如果需要扩展实例，则使用DTS算法找到最佳配置，以最大限度地减少内存使用量，同时保证它们的延迟 SLO。找到最佳配置之后，调度程序根据最大张量相似度将该实例分派到合适的服务器上。







