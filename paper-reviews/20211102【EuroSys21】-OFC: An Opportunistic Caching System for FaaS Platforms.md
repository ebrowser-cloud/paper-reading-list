By Jiabin Chen

# OFC：FaaS 平台的一个机会缓存系统
[OFC: An Opportunistic Caching System for FaaS Platforms](https://dl.acm.org/doi/10.1145/3447786.3456239)
---

* 论文发表在 EuroSys 21。由于 FaaS 的无状态性，它们需要经常与外部数据存储进行交互，这限制了 FaaS 的性能。文章旨在缓解这个问题，提出了一个透明、垂直和水平弹性内存缓存系统 OFC。

* 文章通过利用两种常见的被浪费的资源，经济高效地实现了这个缓存系统。

  * 大多数云租户过度配置其函数的内存资源，因为它们的占用空间严重依赖于输入。
  * FaaS 厂商使函数沙盒会保持几分钟的活动状态，以避免冷启动。

  OFC 使用针对典型函数输入数据类别（例如多媒体格式）调整的机器学习模型，估计每个函数调用所需的实际内存资源，并囤积剩余容量以供给缓存。

* 通过使用一组不同的工作负载，文章发现 OFC 将单阶段函数和多阶段函数的执行时间分别提高了 82% 和 60%。

## 论文作者
* [Djob Mvondo](https://djobiii2078.github.io)：法国雷恩第一大学副教授；IRISA 实验室；主要研究方向有虚拟化、云计算和操作系统。
* [Mathieu Bacou](https://bacou.wp.imtbs-tsp.eu/)：南巴黎电信学校助理教授；Parallel and Distributed Systems (PDS) group；主要研究方向有操作系统、虚拟化、云计算、serverless 和 FaaS 以及资源和电源管理。
* [Alain Tchana](http://perso.ens-lyon.fr/alain.tchana/)：里昂高等师范学院；LIP laboratory；主要研究方向有虚拟化、虚拟化和云计算。

## 论文贡献

1. 利用机器学习准确预测 FaaS 系统中函数的内存需求。
2. 利用过度配置的内存和保持函数沙盒活跃的内存资源来实现一个性价比好、具有弹性和容错的缓存系统 OFC。
3. 通过使用一组不同的工作负载评估 OFC，实验结果表明单阶段函数和多阶段函数的执行时间分别降低了 82% 和 60%。

## 论文动机

### 问题引入：

* 在过去几年中，FaaS 深刻地改变了设计、操作和计费云原生应用程序的方式。它允许租户只关注应用程序代码，应用程序代码被组织为一组无状态函数，用户只需上传这些函数，这些函数将由事件自动触发。
* 由于其无状态性质，大多数函数遵循提取转换加载（ETL）模式。这意味着函数首先（E）从远程持久存储服务（例如，对象存储，如AWS S3）读取数据（例如，图像），然后（T）执行一些计算（例如，图像模糊），最后（L）将结果写入远程存储。

* 当前 FaaS 平台有两个主要性能限制：调度延迟和函数的执行延迟。本文关注后者。
* 当前的 FaaS 平台通常由两个截然不同的层组成：计算基础设施和远程存储后端。这种解耦是一把双刃剑：FaaS 应用程序受益于无与伦比的灵活性，但受到后端对持久或共享瞬态的任何访问（E&L 阶段）的延迟和吞吐量限制的影响。

### 先前解决方案：

1. 专门使用由云租户预订、配置和调整的资源，以提供更好的存储性能，但这与无服务器模式的预期好处不符。
2. 通过利用特定于平台的假设和功能，实现了函数实例之间的直接和高效通信。
3. Cloudburst 平台通过每个 worker 的数据缓存改进了数据的局部性，这些数据缓存使用特定的、无协调的一致性协议与专用存储后端交互。Cloudburst 需要在每个 worker 上手动或者静态地提供专用缓存资源（特别是内存）。

## 文章解决方案：OFC

### OFC

* 文章提出了OFC（机会 FaaS 缓存），这是一种机会主义的基于 RAM 的缓存系统，通过减少 E&L 延迟来提高单阶段函数和多阶段函数的执行时间。

* OFC 以一种经济高效的方式为云厂商实现了这一点，云租户无需额外努力（管理、代码修改），数据一致性和持久性保证也不会降低。

* 对 FaaS traces（包括AWS Lambda存储库）的分析表明，用户倾向于过度提供保证其函数的内存容量。此外，为了加速函数实例，FaaS 平台通常会让函数沙盒保持活动状态几分钟（例如，OpenWhisk 中有 10 分钟，Azure 中有 20 分钟），以便为相同函数代码相关的未来调用提供服务。

  ![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.1041b3keik74.png)

* OFC 重新调整了由于内存过度调配和沙盒保持活动而浪费的内存的用途，OFC 机会主义地聚合来自所有工作节点的这些空闲内存块，为 E&L 阶段提供分布式缓存系统。

### 实现 OFC 存在的问题和解决方案：

问题

1. 如何在函数调用粒度上准确预测沙箱所需的内存量（冷启动或者热启动）？

2. 如何构建一个垂直可伸缩的缓存系统，其容量（向上/向下）伸缩延迟甚至与短函数执行（在秒或毫秒范围内）相等？

3. 如何透明（未修改的应用程序代码）、高效（一致性和性能保证）和可靠（持久性和容错）地管理缓存系统？

对应解决方案

1. 使用机器学习预测沙箱所需的内存量，文章采用了决策树算法。
2. 解决问题 2 的难点在于当工作节点缺少内存时，缓存系统的规模缩小。文章采用了两种策略：
   * 首先，通过尽早收回将来不太可能重用的数据来减轻缓存系统的压力。
   * 其次，实现了一个优化的对象迁移算法，允许将热对象保留在分布式缓存的另一个工作节点中。
3. 利用了机器学习以及多个系统和数据缓存策略来解决问题 3：
   * 首先，OFC 利用机器学习构建了另一个模型，该模型输出一个估计值，指示缓存是否会改进性能，只有在缓存的预测性能会提到提升时才会进行缓存。
   * 其次，一旦流水线函数执行结束，由流水线函数生成的中间输入或者输出数据将从缓存系统中删除（但不会持久化到远程存储）。
   * 最后，为了实现剩下的目标，OFC 实现了几种其他技术：
     * 使用辅助 FaaS 函数实现远程存储上的异步数据持久化。
     * 改写 FaaS 调度器实现局部性（函数最好在承载缓存数据副本的工作节点上运行）。
     * 使用“影子对象”（即新对象版本的占位符）在远程存储后端进行一致性管理。

### OFC 架构

![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.3grncjvgwedc.png)

* 上图为 OFC 的架构图，其基于 OpenWhisk 的架构，其中填充有颜色部分是 OFC 在 OpenWhisk 基础上新添加的组件。
* 函数调用流程：
  * Controller 收到一个请求。
  *  Controller 请求 predictor，predictor 返回两个值，一个是预测的内存大小 Mp，另一个是预测是否应该缓存的布尔变量 shouldBeCached（预测缓存是否能获得显著性能提高，是则缓存，否则不缓存）。
  * Controller 选择一个合适的 invoker 来服务这个请求。
  * invoker 节点使用 Sizer 组件，根据 Mp 调整沙箱的内存使用大小。
  * CacheAgent 调整 cache 存储的大小（收缩或者扩张）。
* 函数执行期间的读写请求会被 Proxy 组件捕捉，并使用 rclib 将读写过程重定向到缓存系统，这个过程对用户来说是透明的。OFC 利用 RAMCloud 来实现缓存系统。
* Proxy 在 FaaS 平台中注入一个 Persistor 函数，以将缓存数据异步持久化到 RSDS 存储中。

#### 机器学习模型

* 机器学习算法选择要求

  * 需要进行模型的更新：
    * 函数刚被上传的时候，模型是空白的。
    * 对于不好的预测，模型需要更新矫正。
  * 模型输出：
    * 预测内存输出是一个范围。OWK 定义了一系列允许的内存分配（默认为 0 到 2 GB）。我们将其划分为多个区间，以便将模型表述为分类器，从而更容易进行预测。因此，要分配的内存量是预测间隔的上限。
  * 模型输入：
    * 每个函数输入都不同，所以需要为每个函数训练一个模型。
  * 预测速度：
    * 冷启动平均延迟在 100ms 量级，热启动平均延迟在 10ms 量级。
    * 文章将预测时间目标定在 1ms 量级。

  文章选择 J48 决策树算法（C4.5 的 Java 实现），16MB 的间隔，从 0 到 2GB。
* 缓存增益预测
  * 提取和加载数据的时间占总时间的一半以上，在这种情况下进行缓存被认为是性能提升显著的。
  * 使用 J48，并使用预测内存相同的特征，为每一个函数训练一个模型。
* 预测误差管理
  * 预测不足会导致交换甚至被 OOM 杀死。

  * 为了避免上述情况，在未到达成熟标准之前，不会用其来进行内存预测。

  * 成熟标准：

    * 假设 Ck 为 k-th 分类区间，更大的 k 对应于更高数量的预测内存， k\* 是真实间隔的索引。
    * 预测的 90% 是正确或者更高的预测（EO-predictions），50% 资源预测不足（under-predictions）在 k\*-1的区间内。

  * 当满足成熟标准之后，为进一步降低 under-predictions 的问题影响：
    * 使用预测的下一个区间作为预测值。
    * 如果一个调用因为 OOM 而失败，立即将内存提升到租户的设置值并进行重试。
    * OFC 在调用期间会监测它们的实际调用内存使用。无论何时监测到内存的耗尽，都会快速将这个错误考虑在内，对模型更正。

    * OFC 还尝试动态检测高内存压力的沙盒并动态提高其内存上限（仅对运行时间超过 3s 的调用进行检测）。
* 重新训练
  * J48 并不是一个增量训练的模型，当有新数据时，需要重新训练所有数据。
  * 文章通过维持一个小但是有价值的数据集，在达到成熟度标准后，仅加入预测过低或者过高的数据进行重新训练。文章还给预测过低的数据以更高的权重进行训练。

#### 缓存设计

* 缓存存储
  * 缓存策略
    * 对于一个函数调用，对象只会在满足以下两个条件下考虑进行缓存
      * 对象的大小必须小于最大限制对象大小，文章中设置为 10 MB。
      * 预测的缓存性能提升必须显著。
    * 多阶段函数场景下，中间函数的输出缓存会在最后一个函数运行完成后进行释放。
    * 多阶段函数的最后一个函数或者单阶段函数的输出会在持久化到远程存储后被缓存马上丢弃。
    * 为了回收更多的空间，cacheAgent 周期性丢弃最近未使用的对象。
  
* 垂直扩展
  * OFC 适时囤积每个 Invoker node 上已预订但是未使用的内存。

  * 文章的设计定量考虑了三个方面：

    * 在整个（分布式）OWK 基础设施中处理空函数所需的端到端时间，范围为 8 毫秒。
    * 动态重新配置（即放大或缩小）RAMCloud 实例的内存池所需的时间，范围为几十毫秒。
    * 调整沙盒的资源限制所用的时间（在使用 Docker 的 OWK 中，这是对 cgroup Linux 子系统的系统调用），其范围为24毫秒。

  * 在一个 Invoker node 上，负载变化带来了两个挑战：
    1. 考虑到大多数函数的内存消耗是对输入敏感的，沙盒在其生命周期内的内存需求可能会有很大的波动。
    2. 意外的负载峰值可能需要快速释放部分（甚至全部）缓存资源，以适应要求更高的请求。

  * 解决挑战的方法：
    1. 对每次调用都调整沙箱的内存。通过异步执行内存调整来优化关键路径，但这存在内存容量冲突的风险，可能造成调用失败。这种情况在预测不足和突发负载的情况下更为严重。为了减少上述情况发生的几率，每个 Invoker node 都提供了一个松弛的内存池，初始为 100 MB，每 120s 调整一次。

    2. 函数的最终输出，在持久化到 RSDS 后但是还未释放的，马上释放 cache。如空间任然不够，则根据 LRU 释放 cache，直到空间足够。同时，触发脏输出对象的写回操作，完成后丢弃它们。cacheAgent 通过将热输入对象的主（内存中）副本卸载到另一个 RAMCloud 存储节点，尝试将其保留在缓存中。

* 请求路由

  文章的目标是：

  * 在调用方节点之间实现良好的负载平衡（关于函数调用以及缓存服务产生的负载）。
  * 限制缓存管理开销（例如，内存资源调整和节点之间缓存对象的传输）。
  * 改进数据局部性。

  为实现上述目标，文章修改了 OWK 的 Loadbalancer 组件用于路由函数调用请求的策略。

  * 与原始设计类似，对函数 F 的请求总是路由到 F 的一个空闲（热）沙盒（如果有的话）以避免冷启动，否则将立即创建一个新沙盒（以避免在长时间运行的请求后排队等待）。

  * 如果必须创建一个新的沙盒，那么目标 invoker 节点最好是当前在其本地 RAMCloud 存储实例中承载 master（in-memory）缓存副本的节点（如果存在并且有足够的资源）。
  * 如果有多个可用沙盒，路由算法通过降低优先级顺序使用以下标准：
    * 沙箱的当前内存容量与新调用的预测容量之间的差异（最小的差异是首选）。
    * invoker 节点上的可用内存容量（如果容量必须增加）。
    * 数据的位置（首选与请求对象位于同一位置的沙盒）。
    * 沙箱的闲置时间（最好是最近使用的沙箱，以便旧沙箱最终可以暂停使用，并在剩余时回收）。

## 实验
实验设置：
* 6 台物理机器，通过运行 Ubuntu 16.04.7 LTS 的10Gb/s 以太网交换机相连。
* 物理机配置： 
  * 2 Intel Xeon E5-2698v4 CPUs (20 cores/CPU)
  * 512GB of RAM
  * one Intel Ethernet 10G 2P X520 Adapter
  * one 480GB SSD
* 使用一台物理机承载 OFC 组件 ModelTrainer 和 Controller，另一台物理机用来作为存储系统，其它的机器都作为 FaaS worker 节点。
* 设置 8 个租户，并将每个租户与不同函数相关联。
* RSDS 使用 Swift。
* 运行FaaSLoad 30 分钟。函数调用间隔遵循指数规律𝜆 = 1/60，平均调用间隔为1分钟。
* 内存配置：
  * naive：预留 OWK 允许的最大内存大小（2 GB）。
  * advanced：根据以前的运行情况，预留函数已使用的最大内存量。
  * normal：预留 advanced 租户选择的 1.7 倍内存大小。

baseline 设置：
* OWK-Swift，即没有缓存。

实验结果：

![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.487ftjnc6im8.png)

* 上图表示三种场景（不同的内存配置）中每个函数的所有调用的执行时间总和。

* 在每种情况下，OFC 的表现总是优于 OWK Swift，改善率在 23.9% 到 79.8% 之间（平均 54.6%）。

* 对于大多数函数，naive 租户的结果略好于 advanced 租户（差异为 2–3%），因为在 naive 情况下，OFC 的内存容量比 advance 租户大（从而产生更多缓存命中）。下图展示了 OFC 的缓存大小在三次实验中的变化情况。

  ![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.nuebk7s9z0.png)



## 其他
文章中提到的缺点及未来工作：
* 当 worker 节点缺少内存时，OFC 的缓存会导致函数设置延迟。（缓存收缩的调整时间和增加容器内存的时间在某些极端情况下可占执行时间的 50.4%。文章作者相信，这种情况很可能是罕见的。文章的数据缓存策略会尝试尽早从缓存中释放未使用的数据。）

文章相关工作：

FaaS performance bottlenecks

* Cloudburst，VLDB 2020，ccf A
  * 使用与函数执行器位于同一位置的缓存，并无缝地支持现有函数。
  
  * Cloudburst 利用宽松的数据一致性来实现最大的可伸缩性和可用性，而 OFC 则致力于提供更强的一致性和持久性保证，以支持更广泛的用例集。
  
    ![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.3zeabu81r7ls.png)
  
* Faasm，USENIX ATC 2020，ccf A
  * 使用类似于分布式共享内存的抽象，通过在工作节点内部和工作节点之间使用共享内存，加速函数实例之间的数据移动。
  
    ![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.7aire4pc7n0.png)
  
* Infinicache，FAST 2020，ccf A
  
  （是 FaaSNET 同一个作者）
  
  * 利用 FaaS 沙盒及其保持活动的策略来实现弹性内存缓存。
  
    ![image](https://raw.githubusercontent.com/JBinin/Image-hosting/master/OFC/image.42hgasmlyri8.png)
  
* Lambada，SIGMOD 2020，ccf A
  * 是一个专门的框架（用于交互式数据分析）。
  * Boxer 改进了 Lambada，实现了函数实例之间的直接网络通信（以及直接数据交换），这也可以为更广泛的用例带来好处。
  * OFC 的方法即使在函数之间可以进行直接通信时仍然有用，因为它加速了 ETL 模式的 E 和 L 阶段。
  
* FaaSCache，ASPLOS 2021，ccf A
  * 微调 FaaS 平台的保持活动策略。这种方法既不解决数据缓存和交换问题，也不缓解输入可变性造成的内存浪费，OFC 是对这种方法的补充。

