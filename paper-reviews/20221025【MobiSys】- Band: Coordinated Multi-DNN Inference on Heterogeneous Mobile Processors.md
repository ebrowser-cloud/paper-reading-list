# BAND：在异构移动处理器上的多DNN推理

**[Band: coordinated multi-DNN inference on heterogeneous mobile processors](https://dl.acm.org/doi/10.1145/3498361.3538948)**  

解决问题：在异构移动处理平台上如何高效的进行DNN推理。

本文介绍了BAND，移动DNN调度框架，BAND主要贡献：1.将模型划分成若干个子图；2.根据调度策略动态的选择合适的子图运行。

在移动平台上的评估结果表明，对于涉及多个 DNN 的单应用工作负载，我们的系统优于最先进的移动推理框架 TensorFlow Lite 高达 5.04 倍。对于由延迟关键 DNN 请求组成的多应用场景，BAND 的 SLO 满意度高达 3.76 倍。



## 论文作者：

* Joo Seong Jeong：首尔国立大学
* Jingyu Lee：首尔国立大学
* Donghyun Kim：首尔国立大学
* Changmin Jeon：首尔国立大学



## 论文动机：

深度学习算法的快速发展，很多移动应用要用DNN来解决问题。现在的移动DNN平台都是异构的，有各种移动处理器。现有的移动深度学习框架不能很好的利用异构移动处理器处理DNN推理请求。框架为DNN指定一个快速处理器，该模型的实例都固定用这个处理器处理。当这个DNN出现多个推理请求时，其他处理器也不会被用到。BAND在协调DNN推理请求过程中出现了三个挑战，分别是：

* 处理器争用：多个 DNN 尝试使用相同的处理器，这导致了很大的延迟开销。
* 后备运算符的争用：不同的移动平台配备了不同组合的处理器，并非所有处理器都支持DNN的各种操作。当运行后会运算符时，会默默地回到 CPU。不协调的回退会阻止其他操作员访问原本空闲的处理器；其他非 CPU 加速器不被视为处理后备运算符的选项。
* 性能的不确定性：因为移动平台的动态电压调整机制，会根据程序的计算需求调整电压和频率，这使得难以预测推理时延。



### 架构设计：

BAND主要包含三个部分：

* model analyzer：模型划分为子图。
* central scheduler：为任务选择子图和处理器。
* per-processor workers：在各自的处理器上执行子图。

<img src="https://s3.bmp.ovh/imgs/2022/11/06/38bdb9b36fb15a93.png" alt="image-20221106192640953" style="zoom:50%;" />

### Subgraph Partitioning

* 目标：准备一组不同的子图，以便可以在运行时考虑许多可能的计划
* 现有的框架只用一个特定的调度：TensorFlow Lite 根据处理器是否支持运算符对相邻运算符进行分组。该处理器支持的运算符始终在该处理器上运行，而回退运算符始终在 CPU 上运行。
* Units and subgraphs：
  * Units：一个单元是一组相邻的运算符，它们都由完全相同的一组处理器支持。
  * subgraphs
    * 子图生成过程首先为每个Units制作单个单元子图。
    * 如果相邻的子图至少有一个共同支持的处理器，则可以将它们合并为更大的子图。【之前的小子图也会被保存下来】
* Subgraph usage and memory：有些子图利用率很低，浪费内存。

### Subgraph Scheduling

#### Scheduling

<img src="https://s3.bmp.ovh/imgs/2022/11/07/b4cdff7a4a48f22a.png" alt="image-20221107100454534" style="zoom:50%;" />

* WorkFlow

  * 对于每个推理请求，调度器会生成一个job，并用请求模型的Unit对其进行标记，然后该job会放入一个队列。
  * 当一个job放入队列或者worker执行完一个subgraph之后，会调用调度策略。
  * GETNEXTJOB() 是按照调度策略来选择下一个执行的job。
  * 对于选中的job，调度策略也会选择一个subgraph和processor来处理。

* Selecting a job from the job queue

  根据设定的调度策略来选择下一个工作。Figure 12 是使用LST的策略。

  * 考虑所有有效的子图序列并选择具有最短延迟的序列。【有效子图序列是覆盖了所有的Unit一次的子图序列】

  * 如果有多个子图序列是一样的，但是由于子图在不同的处理器上进行时间不一样，则会选择最短需要的时间。

  * 最后调度程序会选择具有最短的松弛时间的Job，并选择其最短预期时间的子图进行执行。

  * Scheduling the remaining units of a job.

  * 调度策略未选择的job的剩余Units将重新排入作业队列。

  * 策略可以通过 ENQUEUEREMAINING() 函数自由选择将剩余单元排入队列的位置

* Handling processors with the thermal shutdown.

  热节流而变得不可用，我们会禁用其工作线程并将其排除在调度之外。

  工作人员正在处理的子图被重新排入调度程序，以便另一个工作人员可以接管。

  BAND 轮询禁用的工作程序以在处理器再次联机时通知调度程序。

#### 执行时间分析

* 当模型首次注册到 BAND 时，系统会运行模型几次以检索基线执行时间值，并根据基线执行时间估计模型子图的执行时间。之后，不断进行在线调整以反映当前的工作量模式。
* BAND 会为每个可用处理器依次运行模型的最大子图并记录延迟。
* 为了避免严重中断正在运行的作业，BAND 会一直等到处理器上的当前作业完成，然后暂停相应的工作程序，以便调度程序不会在该处理器上调度后续作业。
* 在此暂停期间测量子图在处理器上的执行时间后，立即通知调度程序该工作人员再次可用，并且调度程序正常进行。
* 在检索每个可用处理器的最大子图的执行时间值后，BAND 测量每个子图的浮点运算 (FLOP) 数量，以及输入和输出张量大小。假设每一个子图的运行时间正比于 $FLOP_s + \beta * tensor_{size}$ 。从而推测出每一个子图的执行时间。
* 最后，使用简单的线性平滑函数在运行时调整子图的延迟曲线。 $time_{profiled} = \alpha * time_{new} + (1 - \alpha)*time_{profiled}$ 【经测试：$\alpha = 1$ 性能比较 好】



## 实验评估：

### 实验环境设置：

实验平台：

* Google Pixel 4（高通骁龙 855 + Google Edge TPU）
* 小米红米 K40 Gaming（联发科天玑 1200）
* 三星 Galaxy S20（高通骁龙 865）

调度策略：

* TensorFlow Lite (TFLite)：未修改的 TensorFlow Lite 2.3.0。模型的第一个后备运算符之后的所有运算符（包括非后备运算符）都在 CPU 上运行。
* TFLite+MaxPartition (TFLite+MP) ：它不断在 CPU 和另一个处理器之间来回切换，分别运行后备运算符和非后备运算符。
* BAND (No Subgraph)：没有子图调度的BAND，主要优点在于，对于多个相同的DNN请求，可以根据当前处理器的状态选择合适的快速处理器。
* BAND 

工作负载：

* EagleEye
* EagleEye(Crowded)
* Distream



### 实验一：在单个工作负载下，BAND与其他三个策略的对比表现

![image-20221017152219721](https://s3.bmp.ovh/imgs/2022/10/17/c7e158775244b973.png)

分析：

* 对于EagleEye这个负载
  * TFLite+MP 始终比基本 TFLite 更快，因为 TFLite+MP 在非 CPU 加速器上运行更多运算符。
  * 在处理器上动态放置模型的没有子图的BAND也对性能有了进一步的提升
  * 最后增加了子图分割和调度机制的BAND表现最好
* 对于Distream这个负载
  * TFLite+MP并没有比TFLite获得更好的性能，这个是因为频繁的设备转换超过了使用非 CPU 加速器所获得的性能提升，从而导致更差的 FPS。
  * 但是BAND仍然可以获得更好的性能。



### 实验二：分析Profiling的性能

<img src="https://s3.bmp.ovh/imgs/2022/10/17/fcf94143cff8bdc3.png" alt="image-20221017153238813" style="zoom: 33%;" />

* Static + offline 与 BAND相比，其预测时延和策略时延相差较明显，预测不够准确。
* Dynamic + offline 先静态测量再动态修改和BAND两个配置方法总体效果都不错，但是需要耗费更长的时间来测量。



### 实验三：对比没有子图的BAND，深度分析子图机制的性能

![image-20221017155646946](https://s3.bmp.ovh/imgs/2022/10/17/8dd52adaac7050dc.png)

* 一般来说，BAND 的子图级执行优于模型级执行 BAND（No Subgraph）【主要原因是：当处理器出现后备运算符时，调度程序可以让加速器处理后面的请求，从而使处理器不会保持空闲状态。】
* 对于RetinaFace这个模型，因为在所有的处理器上的执行时间差不多，所以有没有子图对其影响并不大。除此之外，因为子图主要是对有fallback operator的模型优化比较大，具有较少后备运算符的模型从子图调度中受益较少。



### 实验四：多工作负载情况下，对比TFLite策略，分析BAND的性能表现

实验：将该模型单独运行在速度最快的处理器上的 99latency 乘以不同的值作为 SLO，来测量不同模型在两个调度算法下的SLO满足率。

<img src="https://s3.bmp.ovh/imgs/2022/10/17/d323702c66bba30c.png" alt="image-20221017161609749" style="zoom:50%;" />

分析：总体来看Band仍然表现的最好。multi-app的情况下，Band会利用其他有较大松弛时间的模型最大限度的减少违规。



### 实验五：分析BAND的功率消耗

<img src="https://s3.bmp.ovh/imgs/2022/10/17/1a64d1fc69d1e6d1.png" alt="image-20221017162550244" style="zoom:50%;" />

从table2中可以看到对于EagleEye(Crowded) 这个工作负载，Band消耗了7.99W，只比TFLite+MP增加了5.1%，但是却比TFLite+MP处理帧增加了2.1倍。



## 相关工作：

* 多个DNN在单个处理器：
  * DeepEye,MCDNN,NestDNN : 模型压缩，对模型进行修改
  * Layerweaver : 利用 NPU 处理元件 (PE) 阵列上的计算和 DRAM 上的内存访问可以同时发生的事实，以实现高资源利用率。
  * Masa : 重叠了计算和内存访问，更多的是集中在与内存方面。
  * DART :在异构平台上提供  real-time和best-effort任务的调度，但是这个是在服务器平台，不是在移动处理器平台，服务器平台可有提供任务抢占和CUDA context共享。
* 单个DNN在多个处理器：
  * μLayer ：将 DNN 层划分为通道（用于 conv 和 FC），并在 CPU 和 GPU 之间分配通道的计算。
  * AsyMo ：将矩阵乘法运算划分为子任务。
* 多个DNN在多个处理器：
  * LEO,Starfish : 用于在传感和连续视觉应用的类似任务之间进行计算和内存共享的集中式框架。
  * MobiSR,EagleEye :内容感知优化，可有效处理任务。



## 其他：

### 文中提到的缺点和未来工作：

* 子图划分之后，存在一部分子图利用率很低，或者可能永远不会被用到，这样的子图不必要地占用内存而不使系统受益。如何从这个方面进行内存优化？
* 热节流处理：与 BAND 排除某些处理器的后节流机制相比，预先检测到这种过度功耗的预节流机制对于维护长时间运行的服务可能更有效。

### 文中疑问：

<img src="https://s3.bmp.ovh/imgs/2022/11/07/7997a101a108aac9.png" alt="image-20221107103943023" style="zoom: 50%;" />

1. 这个 beta 的计算是否出现错误？ 为什么加速器是 1000/bytes CPU是 10/bytes
2. 在每个加速器上只执行最大的子图，那么对于有些子图可以在该加速器上执行，也不包含在最大的子图中，这些子图的预计执行时间怎么获得？
3. 模型调度采用了动态规划方法快速的找到调度的子图，这个动态规划方法是怎么使用的，没有具体解释清楚。

### 总结：

本文主要是通过将模型划分成子图来优化后背运算符的调度。所以其优化场景主要是集中在：

* 出现较多后备运算符的模型。
* 在CPU和其他加速处理器上的运行速度有较大区别的模型。

模型划分：其子图划分的方案是根据两个相邻的子图是否共享相同的快速处理器，合并要求过于简单，所以导致大量低利用率的子图产生。

模型调度：调度过程中，其采用了动态规划方法来优化时间。

子图执行时间配置：在快速处理器上运行最大的子图，根据这个子图去估计其他子图运行时间。

