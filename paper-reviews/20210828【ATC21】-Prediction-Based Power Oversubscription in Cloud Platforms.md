# Prediction-Based Power Oversubscription in Cloud Platforms

"Prediction-Based Power Oversubscription in Cloud Platforms"时2021年发表在 USENIX Annual Technical Conference 上的论文。

本文认为云供应商可以根据对工作负载性能关键性和虚拟机CPU利用率的预测，通过修改调度虚拟机和机箱管理电源，再利用过度部署策略，充分利用现有资源，在不增加新的数据中心同时相比之前超额部署2倍。


## 论文作者
* Alok Kumbhare ：一名研究软件开发人员，在MSR Redmond的系统研究小组工作。他的工作是在云效率小组内提高大型分布式系统的效率，重点是电源管理和提高数据中心效率。
* Reza Azimi ：PhD in Electrical and Computer Engineering at Carnegie Mellon University 。在无线通信领域有多年的研究和系统工程经验，专门研究使用车辆网络和DSRC的安全应用。
* Anand Bonde ：Senior Research SDE。工作主要集中在系统研究组的云效率方面。
* Ioannis Manousakis : PhD in the Rutgers, The State University of New Jersey, Department of Computer Science。主要研究Azure计算基础设施效率架构，他是Azure数据中心系统设计和软件集成建筑师，Azure液体和浸入式冷却系统负责人，Azure风险和齿轮优化（齿轮优化？？）架构师。
## 论文动机
* 随着IT业务需求量的增长，部署新的数据中心电源功率消耗在数据中心的设计、部署和扩充中的成本消耗占据了重要角色。由于每个基层设施中会配备一个阻断器（？？写上英文），如果发生因电源过度消耗产生的事件，阻断器会关闭部分设备电源，影响整体性能。为了避免促发阻断器，数据中心设计者会考虑机箱额定功率来分配服务器数量，但这种供应方式会导致大量资源无法充分利用，仍然无法避免IT需求量增加时不得不增加新的数据中心，造成成本浪费。
* 过去有提出结合功率限制和过度部署来优化效率减少成本，这个想法是利用实际的服务器利用率和跨工作负载的统计复用，在确保电力消耗保持在阻断器的限制之下，通过向数据中心添加更多的服务器来超额分配交付基础设施。
* 以上所述方式在已知的工作集和服务器部署表现很好，但是对于云平台不太适用。第一，每个服务器跑有很多VM，节流服务器的功率会影响一些关键性VM；第二，VM是动态抵达和离开服务器，这妨碍了预定义服务器优先队列；第三，因为大部分客户不太愿意接受对他们的VM深层监视，所以只能将VM视为黑盒子一样。而且它在集群中采用了被动的和昂贵的VM迁移，而不是利用预测来功率封顶和功率封顶感知的调度。
## 论文贡献
这篇论文认为有许多非关键性工作负载（如批处理工作）可以容忍稍高的功率封顶事件率和/或更深的节流；对关键性工作负载的功率封顶必须控制得更严格。使用预测来识别这些工作负载，并在整个数据中心内谨慎地放置它们，提供了增加超额部署所需的功率松弛和关键性感知。
这篇论文提出了对一些微软Azure原有组件的增加和修改，比如VM调度器、ML模型、机箱管理器、每个VM的功率控制器。这篇论文的主要贡献有：
1. 提出了一个预测关键性VM的算法和一个预测VM利用率的ML模型。这个ML模型，根据历史上的VM到达数据和在这些VM部署后收集的遥测数据来生成这些预测。
2. 根据预测VM关键性和利用率（如图流程2），本文对原有VM调度器的改进后来触发限制事件的发生次数。
3. 提出了一个利用预测关键性的per-VM功率限制系统，其中包括机箱管理器和per-VM功率控制器。机箱管理器循环监视本地PSUs判断是否功率消耗有无超过该机箱预设的阈值，如果有就向per-VM功率控制器发出警报（如图流程4）；per-vm功率控制器接受到警报后会先降低运行非关键性VM的核心频率以降功耗（如图流程5），如果仍然总体消耗功率超过阈值，最终PSUs会警告BMCs（如图流程6），使其用RAPL降低整个服务器功耗至阈值以下。
4. 根据上述的工作内容，提出了一个增加过度部署的方法。
5. 实际运用在Azure的架构里，能有效的减少成本。
6. 总结了一些在Azure实践部署的经验教训。

[![system.png](https://i.postimg.cc/1z7xQcy1/system.png)](https://postimg.cc/CZ8rC8Zc)
## 预测vm关键性--模式匹配算法
本文将预测VM关键性归结为判断那些CPU利用率时间序列呈现24小时周期性的VM为关键性VM。该算法步骤：
1. 它对时间序列进行去趋势化和归一化，使所有日子的VM利用率都在同一大致范围内。去趋势化是根据前24小时的平均数来衡量每个利用率，而正常化则是用整个时间序列的标准差来划分每个利用率。
2. 它通过识别一天中每个时间段（以30分钟为单位）的“典型”利用率来提取24小时的模板，该值为一天中这个时间序列的预处理后的所有利用率的中值。
3. 将模板覆盖在每天的预处理系列上，在排除20%的最大偏差后，计算出每个利用率的平均偏差。
4. 它重复步骤2和3，计算8小时和12小时模板的平均偏差，然后计算两个分数：24小时平均偏差除以8小时平均偏差（称为Compare8），以及24小时平均偏差除以12小时平均偏差（称为Compare12）。如果分数接近于0，则该工作负荷可能是面向用户的。最终，如果一个时间序列的Compare8值低于阈值，它将被归类为面向用户的时间序列。

## 训练ML模型
* 对于利用率的预测，本论文训练了一个两阶段的模型来预测第95百分位的VMCPU利用率，其依据是以前的VM执行产生的标签（VM生命周期中实际的第95百分位的利用率）和在关键度模型中使用的相同的VM特征。由于准确预测利用率是困难的，该论文模型将其预测为4个桶：0%-25%，26%-50%，51%-75%，76%-100%。第一阶段是一个随机森林，预测第95百分位数的利用率是否高于50%。在第二阶段，我们有一个随机森林用于1-2桶，另一个用于3-4桶。

## VM 调度器的修改
原有的VM调度器并没有考虑功率限制这一问题，所以本论文在研究如何选取候选服务器上综合考虑到机箱整体和候选服务器。
VM调度器的修改调整，该论文主要提出三点目标。
1. 更好的部署在不同的机箱上有均衡的功耗分布，以减少封顶事件的数量
2. 在服务器上有均衡的可用功率分布（由非面向用户的VM核心提取），因此控制器可以在事件期间降低功率而不影响关键VM。
3. 调度器必须在打包VM方面保持有效，同时尽量减少部署失败的数量。
* 该算法计算出每个候选服务器的分数，分数越高，服务器就越好。这个分数的计算是机箱分数和服务器分数加权求和得到。
[![algo.png](https://i.postimg.cc/vZpd4dWh/algo.png)](https://postimg.cc/18cdd27V)
* 机箱分数是通过保守地估计其总的机箱CPU利用率来计算候选服务器的机箱得分，即假设所有安排到机箱的VM同时处于各自的第95百分位的利用率。这个值是机箱的VM的第95百分位利用率的总和除以最大核心利用率（机箱中的#核心×100%）。再从1中减去它，因此，利用率低的机箱得到更高的值，并被优先考虑。
[![chassis.png](https://i.postimg.cc/YSz6GbhT/chassis.png)](https://postimg.cc/bGdD400R)
* 服务器分数根据正在部署的VM的类型，对候选服务器进行不同的评分。首先它对面向用户的VM和非面向用户的VM的预测利用率进行独立总结。当面向用户的VM被部署时，计算服务器上非面向用户的VM的利用率比面向用户的VM高多少。对非面向用户的VM做相反的处理。反转是平衡服务器上可用功率的关键。加1除以2确保分数在0和1之间为正值，而更高的值则更好。
[![SERVER.png](https://i.postimg.cc/vBD4Yz9r/SERVER.png)](https://postimg.cc/HcDY3b6n)

## Per-VM 功率限制控制器
* 该控制器主要是在预算功率不足以满足VM消耗情况，限制非关键性VM的核心频率，以保护关键性VM。所以为了保护性能关键性的VM,在修改后的系统中，机箱管理器每隔200ms就会对PSU进行调查，并在机箱功耗接近机箱预算时对每台服务器的带内控制器发出警报。在收到警报后，每个控制器使用每核[DVFS](https://www.sciencedirect.com/topics/computer-science/dynamic-voltage-and-frequency-scaling)对运行不面向用户的VM的内核进行功率封顶。为了(1)两次投票之间的高功率消耗或(2)控制器无法使功率低于预算，使用[RAPL](https://dl.acm.org/doi/10.1145/3177754)对所有内核进行平等的节流，作为备份。
* 为了管理每个VM的功率，该论文使用管理程序的核心分组功能（例如，Xen中的cpupools，HyperV中的cpugroups），将核心分成高优先级和低优先级。将面向用户的VM和I/O密集型VM分配到高优先级的核心上运行，将不面向用户的VM分配到低优先级的核心上。
* 收到来自机箱管理器的警报后，每个VM电源控制器将服务器的耗电量与预算进行比较，如果电流消耗高于预算，控制器立即将低优先级内核的频率降低到最小p状态，即最大频率的一半；频率的降低可能也会导致电压降低。其目的是迅速将服务器的功耗降至极限以下，从而避免启用RAPL，因为RAPL会对所有内核进行节流，影响服务器上所有VM的性能。为了减少对不面向用户的VM的影响，控制器然后进入一个反馈循环，其中每个迭代包括检查服务器功率计和将N个低优先级内核的频率增加到下一个更高的p状态（100MHz步长），直到功率接近预算。它选择能使功率低于该阈值的最高频率。

## 过度部署策略
* 该策略是面对机箱层面，在上述VM调度器中有综合考虑机箱分数与服务器分数，如果要保护关键性VM除了在服务器层面的per-VM功率控制器，也需要在机箱的最适合的最小功率预算值Pmin。因为避免浪费资源而添加新的数据中心，我们可以降低预算值，以获得在同一个机箱中部署更多的服务器，以减少额外机箱成本。而且最小的功率预算值也要避免过多功率限制封顶事件的发生，具体计算Pmin值如下。
* 该策略方法是为每一个硬件的所有机箱计算一个功率预算，在满足可接受的最大封顶限制事件发生的比率以及面向用户和不面向用户的VM可接受的最小核心频率（分别为fminUF和fminNUF），不断调整降低该功率预算，直到找到最低值。
该算法主要步骤：
1. 估计面向用户的VM在分配的核中的历史平均比率(β)和面向用户(utilUF)和非面向用户(utilNUF)的VM中虚拟核的历史平均P95利用率（为什么是P95利用率？？）。
2. 分别给定fminUF和fminNUF，估计在utilluf和utilNUF上通过降低核心频率可以减少多少服务器功率。此步骤生成两条功率消耗曲线(一条曲线表示每次平均利用率)，作为频率的函数。 
3. 按降序排序历史的机箱级功率消耗。 
4. 从最高功率的图纸作为第一个候选预算，并逐步考虑较低的消耗，直到我们找到Pmin。对于每个候选功率预算，我们检查封顶事件的速率不会超过fmaxUF或fmaxNUF。 
5. 为了计算最终的预算，从步骤4增加一个缓冲(例如，10%)到预算，以考虑β的未来可变性  或大幅提高底盘利用率。 




## 实验
本论文实验是在微软Azure上进行。使用一个有12台服务器的机箱，每台服务器包含40个内核，分成两个插座。在其额定频率下，每台服务器的功耗在112W（空闲）和310W（100%的CPU利用率）之间。在这个频率的一半时，每台服务器的功耗为111W到169W。
| 配置参数 |值| 
| ---|----|
|集群配置 | 20架×3机箱×12刀片|
| 叶片（服务器）配置|2x20个核心 |
|VM大小分布|(核心） 1（33%），2（27%），4（21%），8（10%），16(5%), 24 (3%), >=32 (1%)|
|部署规模分布|(#VMs) 1 (39%), 2 (14%), 3-5 (16%), 6-10 (9%), 11-15 (8%), 16-25 (5%), >25(9%)|
|VM的寿命距离|(小时） 1（52%），2（5%），3-5（10%），6-10（9%）10-25(7%), 26-720 (8%), >720 (9%)|
|工作负载类型桶|面向用户（UF），非面向用户（NUF）|
|P95利用率桶|0-25%，26-50%，51-75%，76-100%|
|平均 UF:NUF 核心比率| 4:6|
|平均 UF 和 NUF P95 利用率| 65%（3号桶），44%（2号桶）|
|仿真天数|30|

### 模式匹配算法和ML模型实验
* 评估模式匹配算法，本论文首先将其分类与我们自己对840个工作负载的人工标注进行比较。该图显示了每个工作负载的一个点，其坐标对应于其Compare8和Compare12的值。图中显示，Compare8可以将前两组（算法应保守地将其归类为面向用户）与后两组分开。在Compare8=0.72的垂直条上，所有重要的工作负载都在条的左边，而绝大部分不重要的工作负载都在右边。Compare12并不能很好地区分这些类别。
  
[![compare.png](https://i.postimg.cc/G2YZy4W4/compare.png)](https://postimg.cc/2bkcpS8Y)

* 因为召回率表示正确识别这些VM的概率。实验结果显示了两个高召回率目标（0.99和0.98）的精度和召回率，以确定工作负载是否面向用户。本论文模式匹配算法以更高的精度达到了目标召回率，也就是说，它正确地分类了更多非面向用户的VM。这减少了封顶事件中的性能下降，因为更多的这些VM可以被节流到较低的功率。

[![vsFFT.png](https://i.postimg.cc/y62yGfSC/vsFFT.png)](https://postimg.cc/z3ShbFjx)
* 作为实验对照，该论文使用了GB模型。图表显示，本轮的关键性预测模型对面向用户的VM（Bucket 2）达到了99%的召回率，这对保护这些VM至关重要。GB模型取得了类似的结果。
利用率模型也做得很好，对最受欢迎的桶（1和4）有良好的召回率和精确度（83-93%），对73%的高置信度预测有良好的准确性（84%）。这里，最重要的特征是VM在订阅中95分位数的CPU利用率的平均值，VM在订阅中的平均CPU利用率的平均值，以及VM在订阅中每个CPU利用率桶中的百分比。GB模型实现了类似的准确性，但高置信度的预测较少，中间两个（最不受欢迎的）桶的召回率较低。
[![Ml.png](https://i.postimg.cc/4yy21nMv/Ml.png)](https://postimg.cc/QKLq8NjV)

### Per-VM 功率限制控制器单服务器实验
在单个服务器上实验中，面向用户的应用程序在一个有20个虚拟核心的VM上运行，不面向用户的应用程序同时在另一个有20个虚拟核心的VM上运行，20个虚拟核心的VM上运行。该实验每次执行需要10分钟。
* 该图频率曲线描述了我们的控制器对不面向用户的VM的性能所做的调整。per-VM控制器的目标较低（230W的上限为225W），所以它的耗电量在大多数时候都略低于全服务器上限的情况。
[![PER-VM.png](https://i.postimg.cc/BvNdcWb0/PER-VM.png)](https://postimg.cc/kV61qpW1)
* 下图显示了本实验中封顶对面向用户的应用程序（最左边的10条）的第95百分位延迟和非面向用户的应用程序（最右边的10条）的运行时间的影响。结果显示，全服务器封顶带来了较大的尾部延迟下降，相比之下，本路文的控制器将尾部延迟保持在非常接近无限制的情况下，直到上限非常低（210W）。
[![powercapping.png](https://i.postimg.cc/1XMHbPxm/powercapping.png)](https://postimg.cc/SY2CzBN3)

### Per-VM 功率限制控制器集群实验
* 该实验在一个12台服务器的机箱上实验，运行36个面向用户的应用程序（每份在4个虚拟核心的VM上），以及36ge不面向用户的应用程序（每份在6个虚拟核心的VM上运行）。在VM的放置方面，该论文探索了两个极端的情况。(1) 平衡放置，我们将面向用户的和不面向用户的VM以循环方式放置在服务器上，即每台服务器上有3台每种类型的VM；(2) 不平衡，我们将面向用户的和不面向用户的VM隔离在不同的服务器组。每个实验运行26分钟。
[![chaiss.png](https://i.postimg.cc/sXg1wRJ9/chaiss.png)](https://postimg.cc/t18X4wwY)
如图，显示了VM放置和封顶粒度对面向用户的应用程序的平均95分位数延迟的影响（最左边的4条）和对不面向用户的应用程序的平均运行时间的影响（最右边的4条）。
尽管2450W的预算，但平衡放置下的每个VM封顶使平均尾部延迟与无封顶实验相同。相比之下，在不平衡放置的情况下，每台VM封顶的性能下降程度与全服务器封顶一样多。
* 这些结果表明，我们的控制器在布局允许的情况下保护面向用户的VM。对于不面向用户的应用，全服务器封顶的性能比每个VM封顶的性能略好。更有趣的是，无论封顶的粒度如何，平衡放置的结果都比不平衡放置的结果略差。

### VM 调度器仿真实验
本实验主要参考4个指标，vm调度失败率、平均空服务器比率、机箱平均分数标准方差、服务器平均分数标准方差，并合理估计VM调度算法中α合适的值。
[![image.png](https://i.postimg.cc/YS5MmKsC/image.png)](https://postimg.cc/WDwQQKVB)
图(a)和(b)显示，我们修改后的调度器对α的低值略有影响（对高值根本没有影响），同时无论α如何，空服务器的百分比都略有下降。图(c)和(d)证实，使用我们的策略和预测，负载更加平衡。后面这些图还表明，α的值很重要。α=0产生的机箱利用率平衡比其他值差得多。同时，α=1产生的服务器利用率平衡和现有调度器一样差，而其他值产生的服务器平衡要好得多。
本实验结论α=0.8在这些类型的平衡的重要性之间取得了良好的折衷。

### 过度部署实验
本实验用2018年3个月内Azure的1440个机箱的电力遥测数据来实例化我们的5步超额部署策略。并且使用了2019年4月的VM统计数据，其中非面向用户（utilNUF）和面向用户（utilUF）的虚拟机的第95百分位核心利用率分别为44%和65%，而面向用户的核心在分配的核心中的比例（β）为40%。该实验对比了不同部署策略时机箱预算差额和节约的成本。
|策略 |机箱预算差异（%） |节约成本（$10/W）|
|---|---|---|
|传统的| 0| 0|
|之前最好的方法| 6.2% |$7940M |
|对所有虚拟机的预测，没有UF影响 |11.0% |$1.408M|
|对所有虚拟机的预测，对 UF 影响最小| 12.1% |$1.549M|
|对内部虚拟机的预测，无 UF 影响 |8.4%| $1.075M|
|内部虚拟机的预测，最小的 UF 影响| 10.3% |$1.318M|
|对内部和非高级外部虚拟机的预测，无 UF 影响| 10.6%| $1.357M|
|内部和非高级外部虚拟机的预测，最小UF影响 |12.1% |$1.549M|

正如上述结果所示，本文方法相比之前最好的方法能超额部署和节约成本两倍，目前超额部署和节约成本取决于面向用户与非面向用户的核心比率（β），以及在满足约束条件（emax）的情况下，通过降低频率（fmin）可以从每种类型中恢复的功率。一般来说，β的较高（较低）值会导致较少（较多）的超额部署。

## 经验总结
在实践过程中，作者在部署本文改进的方法在Azure里总结了一些经验。
1. 管理程序对per-VM 功率限制的支持上，没有必要细化到在每个VM核心基础上管理核心频率。
2. 及时刷新VM的关键性标签。
3. 扩展工作集的定义。比如某些虽然被划分为非关键性的VM，但根据客户的不同需求以及时效性，它也可以被归类为关键性。
4. 测量功率限制发生事件的评判标准。
5. 及时关闭VM。有些情况，客户并不希望节流造成影响，所以当VM功率节流发生时，他们会更希望及时关闭虚拟机。
6. 服务器对每个虚拟机管理的支持。本文认为，云供应商更愿意将抽象水平从单个组件提高到整个虚拟机，这将使那些对于生产使用来说过于复杂的进展成为可能，例如功率感知的虚拟机放置，执行每个虚拟机的功率限制，以及根据虚拟机的功率做出节流和关闭的决定。本文作者正在与芯片供应商合作实现这一目标。

## 相关工作
1. 有关利用预测。我们引入了一种新的算法和ML模型来预测工作负载类型和高百分位利用率，力求保护关键工作负载不被限制。
2. 服务器功率封顶限制。大多数工作努力都集中在选择所需的DVFS设置，以满足应用程序执行时的严格功率预算。我们的per-VM功率封顶限制控制器使用每核DVFS和反馈，因此它增加了这一领域的工作，可以在我们的服务器功率上限系统中与每核DVFS结合使用。
3. 集群范围内的工作负载安置/调度。许多工作选择工作负载安置以减少性能干扰或能源使用。不幸的是，它们对云提供商来说往往是不切实际的，依赖于广泛的分析、应用级指标、短期负载预测和/或积极的资源重新分配（例如，通过实时迁移。我们的调度器在安置虚拟机时使用预测，与之前的工作不同，它减少了封顶事件的数量和影响，并增加了功率超额部署。
4. 数据中心的超额部署。其他一些人研究了生产中的分级封顶限制。本论文着重于机箱级功率预算的执行，然而，该技术可以直接地探索，例如对于行级预算的执行，我们可以将虚拟机放在整个行，试图平衡行和服务器。最后，研究人员提出使用能量存储来消除超额订购的数据中心的功率峰值。上述这些方法可以结合起来。

## 结论
该论文提出了基于预测的技术来增加云平台的功率超额部署，同时保护重要的工作负载。本文技术可以将超额部署增加2倍。本文讨论了在生产中部署技术的经验教训。结论是，最近在ML和预测服务系统方面的进展可以释放出云资源配置和管理的进一步创新。
## 思考疑问
* 不同厂家在在云平台服务上，类似本文提出的基于预测的电力过度部署上的区别和优缺点是什么？比如在VM调度方案上，数据中心设计方案等。
* 本文的VM的关键性预测，也就是判断cpu利用率序列周期是否为24小时中的序列处理上有没有更优化准确的方法？如本文的去趋势化只是前24小时的平均数。
* 由于本文是在该论文发表前第一篇运用ML模型（？？是第一篇吗），所以未来在该模型的改进会不会使结果优化？
* 系统各模块间的关系需要解释清楚？？
