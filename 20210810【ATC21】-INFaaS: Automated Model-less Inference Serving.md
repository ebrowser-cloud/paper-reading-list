By AoDong Chen
# INFaas:自动化无模型推理服务
[INFaaS: Automated Model-less Inference Serving](https://www.usenix.org/conference/atc21/presentation/romero)


本文介绍了INFaaS（Inference as a service），一种提供分布式推理服务的自动化(自动选择模型变体)无模型（无需人为指定模型）系统，在该系统中过，开发人员只需为其应用程序指定性能和准确性要求，而无需为每个推理查询指定特定的模型变体。INFaaS能够从已经训练过的模型中生成模型变体，并代替开发人员有效地选择模型变体，以满足不同应用程序的需求。  

与AWS EC2上最先进的推理服务系统相比，INFaaS将吞吐量提高为的1.3倍，延时违反量降为1/1.6，成本降为1/21.6。

## 论文作者
Francisco Romero：斯坦福大学的博士研究生。研究方向主要是视频分析、机器学习推理、和无服务计算。  

Qian Li：斯坦福大学的博士研究生。研究方向主要是异构数据中心中新兴负载的有效管理。  

Neeraja J. Yadwadkar：斯坦福大学计算机科学系的博士后研究员。研究方向主要是在系统中使用并开发机器学习技术，同时研究机器学习系统。  

Christos Kozyrakis：斯坦福大学电气工程和计算机科学教授。研究方向主要是资源高效的云计算、用于新兴工作负载的节能计算和内存系统，以及可扩展的操作系统。

## 论文动机
如今越来越多的应用需要使用机器学习中的模型推理服务，这些提供推理服务的系统常常部署在云平台上。与机器学习中的训练阶段可以离线进行不同，推理服务面向用户，不同用户的需求不同，所需要的推理服务的性能指标也不尽相同。而同一模型经过不同的方式优化后运行在不同的硬件上的最终性能各有差异，因此要为推理查询选择一个合适的模型网络结构、优化方式、硬件平台是一个巨大的挑战；此外，由于推理请求的负载是动态变化的，具有突发性，从始至终使用固定的资源提供负载并不合理，系统采用什么策略以最低的成本来应对这样的变化也是一个值得思考的问题。  

现存的系统都要求开发人员手动指定模型、优化策略和硬件平台，这对开发人员的技术水平要求很高；而对动态变化的负载的响应措施也只是提前设置静态的应对策略，并不灵活，会造成资源的不合理使用。  

因此论文提出了一个用户只需指定模型需要达到的标准（延时、准确率等），由系统自动选择模型变体、自动合理管理资源的供应。该系统主要面对的挑战有以下三点：
- 如何从许多种模型变体（具有相同功能但使用不同网络结构、编程框架、优化方式、硬件资源的模型）中找到正确合适的变体
- 当负载发生变化时，如何调整至合适的资源组合（使用什么模型、多少个、怎样分配硬件资源）使得成本尽可能的低
- 如何做到既不影响模型的性能又能让模型间共享硬件资源  

## 论文贡献
本篇论文提出了一个自动选择模型、管理资源的提供在线推理服务的系统。主要贡献有以下内容：
- 利用论文新提出的模型变体状态机设计了一个合理的模型选择策略
- 结合前人工作和论文新提出的模型垂直缩放策略设计了一个合理的资源缩放器
- 利用论文新提出的模型变体状态机合理地在模型间共享硬件资源

### 模型变体状态机
![](2021-08-11-22-25-07.png)
所有已注册和生成的模型变体的初始状态均为Inactive:即尚未加载到任何worker上。  
一旦一个模型变量实例被加载，它就转换到Active状态，该状态下这些变量实例的服务量少于它们的峰值吞吐量，并由worker的监视守护进程记录其状态。	当变量实例的吞吐量达到峰值时，它们将进入Overloaded状态。  
当在worker上加载的模型变量开始争夺共享资源(例如，缓存、内存带宽或硬件线程)时，就会转为Interfered状态。处于Interfered状态的变量实例服务量仍小于峰值负载，但是延时比正常状态下低。

### 模型选择策略
该系统会在该系统会在两种情况下使用模型选择策略：
1. 推理查询请求刚到来的时候  
   
   算法封装在getVariant方法中，流程如下：  
   在这种情况下，getVariant的输入是查询的需求，输出是变量和服务于查询的worker。getVariant首先检查是否有Active状态下的变体符合查询的要求。因为Active状态下的变体不会导致加载延迟。  
   如果找到这样的变体，INFaaS将查询分派给运行着满足要求变体的worker中负载量最小的worker。  
   否则，INFaaS将考虑处于Inactive状态的变体:  
   getVariant首先查询元数据库，并检索能够满足查询需求的所有可能，然后选择其中加载和推理延迟最低的变体。如果找到这样的变体，INFaaS将查询发送给变体目标硬件利用率最低的worker。  
   否则，由于没有注册或生成的变体能够满足开发人员的需求，INFaaS建议使用能够达到最接近目标精度和延迟的变体。
2. 查询请求负载发生变化的时候（此时与autoscaler共同决策） 
    
   每个worker都有一个Model-Autoscaler，它与模型变量选择策略协商，以做出模型级的自动缩放决策。
   
### 模型自动缩放器
总共有三种放缩方式：  
1. VM级放缩：即通过增加或减少worker的数量来匹配负载量的变化（位于控制器中）
2. model级放缩（位于每台worker中）：
   - 模型水平放缩：即增加或减少模型数量
   - 模型垂直放缩：即对模型进行升级或降级  

考虑成本因素，在不同情况下应选择不同的放缩方式。为了在负载和需求发生变化时得出所需模型变体的类型和数量，同时最小化成本，论文提出了一个**目标函数**：
![](2021-08-11-22-42-41.png)

**约束条件**为：
![](2021-08-11-23-02-23.png)

由于求解该目标函数的最值所耗费时间巨大，改为使用贪心启发式算法来求得尽可能合理的放缩方案，该算法分为扩展与缩减两部分：
- **扩展算法**：  
  首先确定是否需要扩展，判断方法：当worker的饱和吞吐量与worker的当前负载的比值小于某个阈值时，则表明应该进行扩展的操作。  
  worker的负载是所有运行的变体负载之和，每个变体的负载使用batchsize和每秒处理的查询数来计算（怎么计算论文没说）。每个worker的饱和吞吐量为所有模型变体profiled计算所得（即variant-profiled处理后保存在元数据库中的值）的饱和吞吐量之和。  
  然后再次利用getVariant方法，输入为负载量，输出为扩展操作（增加或升级）。  
  该方法首先估计扩展模型变体的成本（先预估要增加多少个变体（怎么预估没说），然后代入目标函数），然后估计升级模型变体的成本（从元数据库找同样模型结构但吞吐量更高的，预估数量（也没说怎么预估），然后代入目标函数），然后选择更便宜的方案。如果该worker无法提供最终选择的方案所需要的硬件资源，worker会与控制器协调，将变体加载到有足够资源的worker中。
- **缩减算法**：  
  模型自动缩放器使用与扩展算法类似的逻辑来判断是否缩减，即判断饱和量与负载量的比值，若比值大于某个阈值则需要缩减。  
  然后定期检查删除一个正在运行的变量的实例或降级到更便宜的变体后是否可以满足当前负载；确定缩减策略后并不立即执行缩减，而是等待一段时间才执行，避免缩减的太快。

### 共享硬件资源
系统在低负载情况下在模型变体间共享硬件资源，一旦发现有模型变体的状态为Interfered，则选择增加worker来增加相应的硬件资源。

## 实验
作者在AWS EC2实例的异构集群上部署了INFaaS。具体为在一个m5.2xlarge实例(8 vcpu, 32GiB DRAM)上托管控制器，在inf1.2xlarge实例(8 vcpu, 16GiB DRAM，一个AWS Inferentia)、p3.2xlarge实例(8 vcpu, 61GiB DRAM，一个NVIDIA V100 GPU)和m5.2xlarge实例上托管worker。所有实例都是Intel Xeon Platinum 8175M cpu，频率为2.50GHz, 系统是带有4.4.0内核版本的Ubuntu 16.04，网速最高为10Gbps
  
Baselines：
- Clipper+:派生自TFS、TIS和Clipper，这个参照物预加载了模型变体，并要求开发人员提前设置变量实例数量。因此作者设置了一个使Clipper+在给定可用资源的情况下达到最高的性能的数量。
- SM+:源自SageMaker和AI平台，该参照物能够水平缩放每个模型变体，但不支持INFaaS引入的模型垂直缩放。
### 实验一 系统整体表现的比较
#### 实验设置：
INFaas启动时配备有5个CPU、5个GPU和2个Inferentia worker  
baselines配备有5个CPU和7个GPU worker。
#### 负载情况：
挑选自推特2018某月某天的真实访问记录。
#### 结论：
![](2021-08-12-10-15-05.png)
与baselines相比，INFaaS实现了更高的性能(1.3倍吞吐量)和资源利用率(5.6倍的 GPU利用率)，以及更低的SLO违规数(降低了50%)和成本(降为1/1.23)
### 实验二 模型选择策略与自动缩放器的比较
#### 实验设置：
只使用CPU的baselines、只使用GPU的baselines和INFaaS共五个实验对象。
#### 负载情况：
论文使用了三种在现实世界中经常观察到的查询负载情况:(a)平稳的低负载(4个QPS)， (b)稳定的高负载(从650个QPS慢慢增加到700个QPS)，和(c)波动负载(在4到80个QPS之间)。模式(a)和(b)对参照物而言是理想情况，因为它们静态地选择了一个变体;作者为每个参照物使用了最具成本效益的CPU/GPU变体。
#### 结论：
![](2021-08-12-10-16-24.png)
- 低负载情况下5个系统都能满足负载要求，但使用GPU的baselines成本-最高，INFaaS与使用CPU的baselines成本相当。  
- 高负载情况下使用CPU的baselines无法满足负载要求，INFaaS的成本最低。  
- 负载波动的情况下，使用CPU的baselines无法满足负载要求，INFaaS的成本最低。
### 实验三 资源共享高效性的比较
#### 实验设置：
作者将Clipper+配置成每个GPU上都保留了一个模型。由于Clipper+需要提前设置worker的数量，作者指定了2个GPUworker。为了公平起见，INFaaS从一个GPU开始，并允许扩展到2个GPUworker。GPU共享开始对性能产生负面影响的负载在不同的模型中是不同的。作者选择了两个在推理延迟、吞吐量和峰值内存方面存在差异的模型变体:Inception-ResNetV2(大型模型)和MobileNetV1(小型模型)。这两种变体都是针对batch-1进行TensorRt优化的。作者每30秒测量一次吞吐量和P99延迟。

#### 负载情况：
为了显示模型的访问频率对资源共享的影响，我们设置了一个场景，其中一个访问频率高的模型提供80%的QPS，另一个提供20%的QPS。这是一个比较常见的频率分布场景。总共75000个batch-1查询，QPS介于50和500 之间。
#### 结论：
![](2021-08-12-10-24-24.png)
INFaaS可以通过增加更多的GPU worker(在本实验中限制为两个)来缓解延迟的增加。此外，INFaaS通过(a)在低负载下时在一个GPU同时运行两个模型，以及(b)仅在检测到争用时添加GPU这两种方法节省了10%的成本。

## 总结
本篇论文结合前人工作，设计了一个对用户而言只需专注于应用需求的提供在线推理服务的系统，合理且方便。但经过组会上的讨论，论文中的模型变体选择虽然实现了自动化，但是使用的算法似乎还是略显臃肿，希望之后能够找到更优秀的算法。
